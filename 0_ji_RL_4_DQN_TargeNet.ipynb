{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great References to study deeper:\n",
    "\n",
    "- David Silver (DeepMind)\n",
    "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html\n",
    "- Richard Sutton\n",
    "http://incompleteideas.net/book/the-book-2nd.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In WinEluk use environment \"cvision\"\n",
    "# In Valta use root environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CartPole-v0: Target Network + \"Q-learning\" + Neural Net (2 layers)\n",
    "- **GOAL**: be able to keep the pole for >195 steps, that is, reward>195\n",
    "\n",
    "- Ref: https://www.nature.com/articles/nature14236\n",
    "-      https://arxiv.org/pdf/1509.02971.pdf\n",
    "      \n",
    "- **PROBLEM** of previous implementation:\n",
    "-   a) samples are highly correlated --> solved with experience replay (OK)\n",
    "-   b) non-stationary distribution --> ?? \n",
    "- **SOLUTION**: create a target network that is updated once a while. It approximates better a supervised learning scenario in which the target is fixed. I.e. the predicted label does not keep changing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main idea: expression to update \"Q\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://i.stack.imgur.com/y2SNU.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://i.stack.imgur.com/y2SNU.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to basically create 2 networks: target and evaluation. Target net is updated once a while with evaluation net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\n",
      "*** Episode 0 ***                 \n",
      " Avg.Reward [last 50]: 0.18, [last 100]: 0.09, [all]: 9.00                \n",
      " epsilon: 0.88, cont_steps= 9\n",
      "Elapsed time:  00:00:00\n",
      "\n",
      "*** Episode 50 ***                 \n",
      " Avg.Reward [last 50]: 105.80, [last 100]: 52.99, [all]: 103.90                \n",
      " epsilon: 0.01, cont_steps= 5299\n",
      "Elapsed time:  00:00:10\n",
      "\n",
      "*** Episode 100 ***                 \n",
      " Avg.Reward [last 50]: 184.60, [last 100]: 145.20, [all]: 143.85                \n",
      " epsilon: 0.01, cont_steps= 14529\n",
      "Elapsed time:  00:00:29\n",
      "\n",
      "*** Episode 150 ***                 \n",
      " Avg.Reward [last 50]: 192.20, [last 100]: 188.40, [all]: 159.86                \n",
      " epsilon: 0.01, cont_steps= 24139\n",
      "Elapsed time:  00:00:50\n",
      "SOLVED! After 190 episodes\n",
      "\n",
      "*** Episode 200 ***                 \n",
      " Avg.Reward [last 50]: 199.92, [last 100]: 196.06, [all]: 169.83                \n",
      " epsilon: 0.01, cont_steps= 34135\n",
      "Elapsed time:  00:01:11\n",
      "\n",
      "*** Episode 250 ***                 \n",
      " Avg.Reward [last 50]: 200.00, [last 100]: 199.96, [all]: 175.84                \n",
      " epsilon: 0.01, cont_steps= 44135\n",
      "Elapsed time:  00:01:34\n",
      "\n",
      "*** Episode 300 ***                 \n",
      " Avg.Reward [last 50]: 198.54, [last 100]: 199.27, [all]: 179.61                \n",
      " epsilon: 0.01, cont_steps= 54062\n",
      "Elapsed time:  00:01:55\n",
      "\n",
      "*** Episode 350 ***                 \n",
      " Avg.Reward [last 50]: 197.44, [last 100]: 197.99, [all]: 182.15                \n",
      " epsilon: 0.01, cont_steps= 63934\n",
      "Elapsed time:  00:02:17\n",
      "\n",
      "*** Episode 400 ***                 \n",
      " Avg.Reward [last 50]: 199.42, [last 100]: 198.43, [all]: 184.30                \n",
      " epsilon: 0.01, cont_steps= 73905\n",
      "Elapsed time:  00:02:40\n",
      "\n",
      "*** Episode 450 ***                 \n",
      " Avg.Reward [last 50]: 187.22, [last 100]: 193.32, [all]: 184.63                \n",
      " epsilon: 0.01, cont_steps= 83266\n",
      "Elapsed time:  00:03:01\n",
      "Average reward: 186.11\n",
      "Average reward (last 100 episodes): 193.50\n",
      "#Episodes until SOLVE:  190\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAskAAAE/CAYAAAC0Fl50AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Debug mode\n",
    "debug = False\n",
    "\n",
    "## Set CPU or GPU device ####\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\") \n",
    "\n",
    "## Set Gym environment ####\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "## Set seeds #############\n",
    "seed_value = 23\n",
    "env.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "\n",
    "## PARAMS ################\n",
    "num_episodes = 500\n",
    "score_goal = 195 # official\n",
    "\n",
    "gamma = 1\n",
    "my_lr = 0.01\n",
    "nhidden = 64\n",
    "\n",
    "replay_mem_size = 50000\n",
    "freq_update_target = 100\n",
    "batch_size = 32\n",
    "\n",
    "egreedy = 0.9\n",
    "egreedy_final = 0.01\n",
    "egreedy_decay = 500\n",
    "\n",
    "## e-Greedy strategy #################\n",
    "def calc_epsilon(nsteps):\n",
    "    epsilon = egreedy_final + \\\n",
    "       (egreedy - egreedy_final)*math.exp(-1.*nsteps/egreedy_decay) \n",
    "    return epsilon\n",
    "\n",
    "class ExperienceReplay():\n",
    "    # works like a buffer of past experience\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity # maximum size of memory\n",
    "        self.memory = [] # list of states\n",
    "        self.position = 0 # keep track where to add the new sample\n",
    " \n",
    "    def push(self, state, action, new_state, reward, done):\n",
    "        transition = (state, action, new_state, reward, done)\n",
    "        \n",
    "        if self.position >= len(self.memory):\n",
    "            self.memory.append(transition) # keep adding while memory is no full\n",
    "        else:\n",
    "            self.memory[self.position] = transition # add state at \"position\"\n",
    "        \n",
    "        self.position = ( self.position + 1 ) % self.capacity # it works as a cuclic counter\n",
    "        \n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        # zip(*list) --> # this will return the states grouped by each variable\n",
    "        # print([i for i in zip([[1,2,3],[4,5,6]])])\n",
    "        # [([1, 2, 3],), ([4, 5, 6],)]\n",
    "        # print([i for i in zip(*[[1,2,3],[4,5,6]])])\n",
    "        # [(1, 4), (2, 5), (3, 6)]\n",
    "        \n",
    "        # random.sample(self.memory, batch_size) --> return random samples of states\n",
    "        \n",
    "        return zip(*random.sample(self.memory, batch_size)) \n",
    "        # If state=[s,a,s',r,done]\n",
    "        # zip(*) will return:\n",
    "        # [(s1,s2,s3),(a1,a2,a3),(s'1,s'2,s'3),(done1,done2,done3)] \n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "# NEURAL NETWORK\n",
    "n_inputs = env.observation_space.shape[0] \n",
    "n_outputs = env.action_space.n\n",
    "\n",
    "# 1) Define the NN architecture\n",
    "class NeuralNet(nn.Module): # self inherits the class nn.Module\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__() # Call parent's init\n",
    "        self.linear1 = nn.Linear(n_inputs,nhidden) # input\n",
    "        #self.linear2 = nn.Linear(nhidden[0],nhidden[1]) # layer 1\n",
    "        self.linear2 = nn.Linear(nhidden,n_outputs) # layer 2\n",
    "        \n",
    "        self.activation = nn.Tanh()\n",
    "        #self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        output1 = self.linear1(x)\n",
    "        output1 = self.activation(output1)\n",
    "        output2 = self.linear2(output1)\n",
    "        #output2 = self.activation(output2)\n",
    "        #output3 = self.linear3(output2)\n",
    "        \n",
    "        return output2\n",
    "    \n",
    "# 2) Define the NN-agent\n",
    "class Qnet_agent():\n",
    "    # 1) Init\n",
    "    def __init__(self):\n",
    "        # a) Architecture\n",
    "        self.nn = NeuralNet().to(device)\n",
    "        self.nn_target = NeuralNet().to(device) ## (NEW) \n",
    "        \n",
    "        # b) Loss\n",
    "        self.loss = nn.MSELoss() # linear regression\n",
    "        # c) Optim\n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(),lr=my_lr)\n",
    "        # (-NEW-)\n",
    "        self.counter = 0\n",
    "        \n",
    "    # 2) Action\n",
    "    def select_action(self,state,epsilon):\n",
    "        # e-greedy with Exploit x Explore trade-off\n",
    "        randx = torch.rand(1)[0]\n",
    "        \n",
    "        if randx > epsilon:\n",
    "            # Exploit\n",
    "            with torch.no_grad(): # more efficient than detach()\n",
    "                state = torch.Tensor(state).to(device)\n",
    "                action_from_nn = self.nn(state)\n",
    "                action_index = torch.max(action_from_nn,0)[1] # 1 for the index\n",
    "                action = action_index.item()\n",
    "                \n",
    "                if debug:\n",
    "                    print('--> Exploit action_from_nn[',action_from_nn,'] action:',action)\n",
    "        else:\n",
    "            # Explore\n",
    "            action = env.action_space.sample()\n",
    "            if debug:\n",
    "                print('--> Explore: action:',action)\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    # 3) Optimize (UPDATED FOR REPLAY)\n",
    "    #def optimize(self,state,action,new_state,reward,done):\n",
    "    def optimize(self):\n",
    "        # only update with have memory enough to sample\n",
    "        if (len(memory) < batch_size):\n",
    "            return\n",
    "        # Get a sample from memory\n",
    "        state,action,new_state,reward,done = memory.sample(batch_size)\n",
    "        \n",
    "        # Convert to appropriate tensors\n",
    "        state = torch.Tensor(state).to(device) # Convert to tensor\n",
    "        new_state = torch.Tensor(new_state).to(device)\n",
    "        #reward = torch.Tensor([reward]).to(device) # since others are list, reward has to be in list format\n",
    "        reward = torch.Tensor(reward).to(device) # reward is a list now!\n",
    "        # We haver to now convert action and reward to tensor!\n",
    "        action = torch.LongTensor(action).to(device) # for integers\n",
    "        done = torch.Tensor(done).to(device)\n",
    "        \n",
    "        #if done: --> done is a vector... so does not make sense to check like this...\n",
    "        #         --> we will use a little trick \"(1-done)\" to control this!\n",
    "        #    target_value = reward # episode is completed\n",
    "        #else:\n",
    "            ## Bellman's equation (- Traditional way -)\n",
    "            # Q[state,action] = reward + gamma*torch.max(Q[new_state])\n",
    "        \n",
    "        # We will use NNet instead to Approx Q (- New way -)\n",
    "        #new_state_values = self.nn(new_state).detach() # leave the grad\n",
    "        # (-NEW-) target network is used\n",
    "        new_state_values = self.nn_target(new_state).detach() # leave the grad\n",
    "        \n",
    "        #max_new_state_values = torch.max(new_state_values)\n",
    "        max_new_state_values = torch.max(new_state_values,1)[0] # max of each column\n",
    "        #target_value = reward + gamma*max_new_state_values\n",
    "        # (1-done) is use as a trick. If done=1 --> target_value = reward\n",
    "        target_value = reward + (1-done)*gamma*max_new_state_values\n",
    "\n",
    "        ## Q-learning: use prediction-error to update Q\n",
    "        # (-- Traditional way --)\n",
    "        # Q[state, action] = (1 - lr) * Q[state, action] \\\n",
    "        #    + lr * (reward + gamma * torch.max(Q[new_state]))\n",
    "        \n",
    "        # (-- New way --)    \n",
    "        # Current state\n",
    "        #predicted_value = self.nn(state)[action] # here we carry grad because it will update\n",
    "        # (version for vector: details on how to set the dimensions!)\n",
    "    \n",
    "        #print('actions:',action)\n",
    "        #print('output net:',self.nn(state))\n",
    "        predicted_value = self.nn(state).gather(1,action.unsqueeze(1)).squeeze(1) # gather return the values of a set of action\n",
    "        #print('reward predictions:',predicted_value)\n",
    "        \n",
    "        # Prediction error\n",
    "        loss = self.loss(predicted_value,target_value)\n",
    "        # Backprop\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # (-NEW-) need clipping the error to [-1,1] makes the net more stable\n",
    "        for param in self.nn.parameters():\n",
    "            param.grad.data.clamp_(-1,1)\n",
    "        \n",
    "        self.optimizer.step() # update params\n",
    "        \n",
    "        # (-NEW-) update target network\n",
    "        if self.counter % freq_update_target == 0:\n",
    "            self.nn_target.load_state_dict(self.nn.state_dict()) # Way to update is to set new params \n",
    "        \n",
    "        self.counter += 1\n",
    "\n",
    "# 3) RL part\n",
    "memory = ExperienceReplay(replay_mem_size)\n",
    "\n",
    "agent = Qnet_agent()\n",
    "steps_total = []\n",
    "all_solved_nepisodes = []\n",
    "solved = False\n",
    "cont_steps = 0\n",
    "tic = time.time()\n",
    "for i_episode in range(num_episodes):\n",
    "    \n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "        cont_steps += 1 # will keep across episodes\n",
    "        epsilon = calc_epsilon(cont_steps) \n",
    "        \n",
    "        # Action is selected\n",
    "        #action = env.action_space.sample()\n",
    "        action = agent.select_action(state,epsilon)\n",
    "        \n",
    "        # Next environment\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        # States: [cart position,cart velocity,pole angle, pole velocity]\n",
    "        \n",
    "        # Update NNet\n",
    "        #agent.optimize(state,action,new_state,reward,done)\n",
    "        memory.push(state, action, new_state, reward, done)\n",
    "        agent.optimize()\n",
    "        \n",
    "        if debug:\n",
    "            print('Step#%d: %s, state:%s'%(step,action,new_state))\n",
    "            #print(info)\n",
    "            env.render()\n",
    "            input(\"\")\n",
    "        \n",
    "        # Prepare next iteration\n",
    "        state = new_state\n",
    "        \n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            #print(\"Episode finished after %i steps\" % step )\n",
    "            \n",
    "            avgReward100 = sum(steps_total[-100:])/100\n",
    "            \n",
    "            if avgReward100 > score_goal and not solved:\n",
    "                print('SOLVED! After %d episodes' %(i_episode))\n",
    "                all_solved_nepisodes.append(i_episode)\n",
    "                solved = True\n",
    "                break\n",
    "            \n",
    "            # Better reporting\n",
    "            interval = 50\n",
    "            if i_episode % interval == 0:\n",
    "                print('\\n*** Episode %i *** \\\n",
    "                \\n Avg.Reward [last %i]: %.2f, [last 100]: %.2f, [all]: %.2f\\\n",
    "                \\n epsilon: %.2f, cont_steps= %d'%\n",
    "                      (i_episode, interval,\n",
    "                      sum(steps_total[-interval:])/interval,\n",
    "                      avgReward100,\n",
    "                      sum(steps_total)/len(steps_total),\n",
    "                       epsilon, cont_steps\n",
    "                      ))\n",
    "                toc = time.time()\n",
    "                print(\"Elapsed time: \", time.strftime(\"%H:%M:%S\", time.gmtime(toc-tic)))\n",
    "            \n",
    "            break\n",
    "        \n",
    "\n",
    "print(\"Average reward: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "print(\"Average reward (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))\n",
    "print(\"#Episodes until SOLVE: \", all_solved_nepisodes[0])\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(steps_total)), steps_total, alpha=0.6, color='green')\n",
    "plt.show()\n",
    "\n",
    "env.close()\n",
    "env.env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
