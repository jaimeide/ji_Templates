{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: Fraud Detection\n",
    "\n",
    "## Goals of this notebook:\n",
    "- Show the use of GridSearch for hyperparameter optimization\n",
    "- Run XGBoost: an efficient decision-tree boosting method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Class to create Matlab-like structure\n",
    "class matlab_like():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context\n",
    "\n",
    "\n",
    "Ref: https://www.kaggle.com/mlg-ulb/creditcardfraud\n",
    "\n",
    "It is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase.\n",
    "\n",
    "## Content\n",
    "\n",
    "The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
    "\n",
    "It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n",
    "\n",
    "## Inspiration\n",
    "\n",
    "Identify fraudulent credit card transactions.\n",
    "\n",
    "Given the class imbalance ratio, we recommend measuring the accuracy using the Area Under the Precision-Recall Curve (AUPRC). Confusion matrix accuracy is not meaningful for unbalanced classification.\n",
    "\n",
    "## Acknowledgements\n",
    "\n",
    "The dataset has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group (http://mlg.ulb.ac.be) of ULB (UniversitÃ© Libre de Bruxelles) on big data mining and fraud detection. More details on current and past projects on related topics are available on http://mlg.ulb.ac.be/BruFence and http://mlg.ulb.ac.be/ARTML\n",
    "\n",
    "Please cite: Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler # to normalize feats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import ADASYN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define my metrics\n",
    "def ji_get_metrics(C):\n",
    "    \"\"\"\n",
    "    C: ndarray, shape (2,2) as given by scikit-learn confusion_matrix function\n",
    "    \n",
    "    Good reading: http://stats.stackexchange.com/questions/49579/balanced-accuracy-vs-f-1-score\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    allres = matlab_like()\n",
    "    \n",
    "    assert C.shape == (2,2), \"Confusion matrix should be from binary classification only.\"\n",
    "    \n",
    "    # true negative, false positive, etc...\n",
    "    tn = C[0,0]; fp = C[0,1]; fn = C[1,0]; tp = C[1,1]\n",
    "    NP = fn+tp # Num positive examples\n",
    "    NN = tn+fp # Num negative examples\n",
    "    N  = NP+NN\n",
    "    TPR = tp / (NP+0.) # Sensitivity or Recall_P\n",
    "    TNR = tn / (NN+0.) # Specificity or Recall_N\n",
    "    \n",
    "    print(' TN=%d, FP=%d'%(tn,fp))\n",
    "    print(' FN=%d, TP=%d'%(fn,tp))\n",
    "        \n",
    "    \n",
    "    precision = (tp/(tp+fp+0.)) # Precision_P\n",
    "    recall = TPR\n",
    "    \n",
    "    allres.C0 = TNR\n",
    "    allres.C1 = TPR\n",
    "    allres.Precision = precision\n",
    "    allres.Recall = recall\n",
    "    allres.BAcc = (TPR+TNR)/2 \n",
    "    allres.F1 = 2*((precision*recall)/(precision+recall))\n",
    "    \n",
    "    return allres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to plot Confusion Matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=0)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        #print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        1#print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standardize or normalize\n",
    "def ji_normalize(df,typeN,selectedCol):\n",
    "    # Return a normalized DF\n",
    "    # Use: dd = ji_normalize(df,'minmax',['colName'])\n",
    "\n",
    "    dfx = df.copy()\n",
    "    if selectedCol:\n",
    "        if typeN == 'minmax':\n",
    "            dfx[selectedCol] = dfx[selectedCol].apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "        else:\n",
    "            dfx[selectedCol] = StandardScaler().fit_transform(dfx[selectedCol].values.reshape(-1, 1))\n",
    "    else:\n",
    "        allcols = dfx.columns\n",
    "        if typeN == 'minmax':\n",
    "            dfx[allcols] = dfx[allcols].apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "        else:\n",
    "            dfx[allcols] = StandardScaler().fit_transform(dfx[allcols].values)\n",
    "        \n",
    "    return dfx\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...         V21       V22       V23       V24  \\\n",
       "0  0.098698  0.363787  ...   -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  0.085102 -0.255425  ...   -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.247676 -1.514654  ...    0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.377436 -1.387024  ...   -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4 -0.270533  0.817739  ...   -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  Class  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) Import data\n",
    "df = pd.read_csv(\"0_data/creditcard.csv\")\n",
    "df.head() # notice that the features are anonymized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If you want to alter/categorize or fix a particular categorical variable (not the case here...)\n",
    "def val_update(val):\n",
    "    if val<0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "#aux = df['V1'].apply(val_update)\n",
    "#aux.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check if there are missing values \n",
    "# --> NO!\n",
    "#df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284807.000000</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94813.859575</td>\n",
       "      <td>3.919560e-15</td>\n",
       "      <td>5.688174e-16</td>\n",
       "      <td>-8.769071e-15</td>\n",
       "      <td>2.782312e-15</td>\n",
       "      <td>-1.552563e-15</td>\n",
       "      <td>2.010663e-15</td>\n",
       "      <td>-1.694249e-15</td>\n",
       "      <td>-1.927028e-16</td>\n",
       "      <td>-3.137024e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>1.537294e-16</td>\n",
       "      <td>7.959909e-16</td>\n",
       "      <td>5.367590e-16</td>\n",
       "      <td>4.458112e-15</td>\n",
       "      <td>1.453003e-15</td>\n",
       "      <td>1.699104e-15</td>\n",
       "      <td>-3.660161e-16</td>\n",
       "      <td>-1.206049e-16</td>\n",
       "      <td>88.349619</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47488.145955</td>\n",
       "      <td>1.958696e+00</td>\n",
       "      <td>1.651309e+00</td>\n",
       "      <td>1.516255e+00</td>\n",
       "      <td>1.415869e+00</td>\n",
       "      <td>1.380247e+00</td>\n",
       "      <td>1.332271e+00</td>\n",
       "      <td>1.237094e+00</td>\n",
       "      <td>1.194353e+00</td>\n",
       "      <td>1.098632e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.345240e-01</td>\n",
       "      <td>7.257016e-01</td>\n",
       "      <td>6.244603e-01</td>\n",
       "      <td>6.056471e-01</td>\n",
       "      <td>5.212781e-01</td>\n",
       "      <td>4.822270e-01</td>\n",
       "      <td>4.036325e-01</td>\n",
       "      <td>3.300833e-01</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.640751e+01</td>\n",
       "      <td>-7.271573e+01</td>\n",
       "      <td>-4.832559e+01</td>\n",
       "      <td>-5.683171e+00</td>\n",
       "      <td>-1.137433e+02</td>\n",
       "      <td>-2.616051e+01</td>\n",
       "      <td>-4.355724e+01</td>\n",
       "      <td>-7.321672e+01</td>\n",
       "      <td>-1.343407e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.483038e+01</td>\n",
       "      <td>-1.093314e+01</td>\n",
       "      <td>-4.480774e+01</td>\n",
       "      <td>-2.836627e+00</td>\n",
       "      <td>-1.029540e+01</td>\n",
       "      <td>-2.604551e+00</td>\n",
       "      <td>-2.256568e+01</td>\n",
       "      <td>-1.543008e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54201.500000</td>\n",
       "      <td>-9.203734e-01</td>\n",
       "      <td>-5.985499e-01</td>\n",
       "      <td>-8.903648e-01</td>\n",
       "      <td>-8.486401e-01</td>\n",
       "      <td>-6.915971e-01</td>\n",
       "      <td>-7.682956e-01</td>\n",
       "      <td>-5.540759e-01</td>\n",
       "      <td>-2.086297e-01</td>\n",
       "      <td>-6.430976e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.283949e-01</td>\n",
       "      <td>-5.423504e-01</td>\n",
       "      <td>-1.618463e-01</td>\n",
       "      <td>-3.545861e-01</td>\n",
       "      <td>-3.171451e-01</td>\n",
       "      <td>-3.269839e-01</td>\n",
       "      <td>-7.083953e-02</td>\n",
       "      <td>-5.295979e-02</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84692.000000</td>\n",
       "      <td>1.810880e-02</td>\n",
       "      <td>6.548556e-02</td>\n",
       "      <td>1.798463e-01</td>\n",
       "      <td>-1.984653e-02</td>\n",
       "      <td>-5.433583e-02</td>\n",
       "      <td>-2.741871e-01</td>\n",
       "      <td>4.010308e-02</td>\n",
       "      <td>2.235804e-02</td>\n",
       "      <td>-5.142873e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.945017e-02</td>\n",
       "      <td>6.781943e-03</td>\n",
       "      <td>-1.119293e-02</td>\n",
       "      <td>4.097606e-02</td>\n",
       "      <td>1.659350e-02</td>\n",
       "      <td>-5.213911e-02</td>\n",
       "      <td>1.342146e-03</td>\n",
       "      <td>1.124383e-02</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139320.500000</td>\n",
       "      <td>1.315642e+00</td>\n",
       "      <td>8.037239e-01</td>\n",
       "      <td>1.027196e+00</td>\n",
       "      <td>7.433413e-01</td>\n",
       "      <td>6.119264e-01</td>\n",
       "      <td>3.985649e-01</td>\n",
       "      <td>5.704361e-01</td>\n",
       "      <td>3.273459e-01</td>\n",
       "      <td>5.971390e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.863772e-01</td>\n",
       "      <td>5.285536e-01</td>\n",
       "      <td>1.476421e-01</td>\n",
       "      <td>4.395266e-01</td>\n",
       "      <td>3.507156e-01</td>\n",
       "      <td>2.409522e-01</td>\n",
       "      <td>9.104512e-02</td>\n",
       "      <td>7.827995e-02</td>\n",
       "      <td>77.165000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930e+00</td>\n",
       "      <td>2.205773e+01</td>\n",
       "      <td>9.382558e+00</td>\n",
       "      <td>1.687534e+01</td>\n",
       "      <td>3.480167e+01</td>\n",
       "      <td>7.330163e+01</td>\n",
       "      <td>1.205895e+02</td>\n",
       "      <td>2.000721e+01</td>\n",
       "      <td>1.559499e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.720284e+01</td>\n",
       "      <td>1.050309e+01</td>\n",
       "      <td>2.252841e+01</td>\n",
       "      <td>4.584549e+00</td>\n",
       "      <td>7.519589e+00</td>\n",
       "      <td>3.517346e+00</td>\n",
       "      <td>3.161220e+01</td>\n",
       "      <td>3.384781e+01</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            V1            V2            V3            V4  \\\n",
       "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean    94813.859575  3.919560e-15  5.688174e-16 -8.769071e-15  2.782312e-15   \n",
       "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
       "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
       "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
       "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
       "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
       "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean  -1.552563e-15  2.010663e-15 -1.694249e-15 -1.927028e-16 -3.137024e-15   \n",
       "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
       "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
       "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
       "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
       "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
       "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
       "\n",
       "           ...                 V21           V22           V23           V24  \\\n",
       "count      ...        2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean       ...        1.537294e-16  7.959909e-16  5.367590e-16  4.458112e-15   \n",
       "std        ...        7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
       "min        ...       -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
       "25%        ...       -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
       "50%        ...       -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
       "75%        ...        1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
       "max        ...        2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
       "\n",
       "                V25           V26           V27           V28         Amount  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
       "mean   1.453003e-15  1.699104e-15 -3.660161e-16 -1.206049e-16      88.349619   \n",
       "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
       "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
       "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
       "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
       "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
       "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
       "\n",
       "               Class  \n",
       "count  284807.000000  \n",
       "mean        0.001727  \n",
       "std         0.041527  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Original data to keep:\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare training & validation & testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train+Validation:(199364, 30), Test:(85443, 30)\n",
      "Index,Train: [ 39873  39874  39875 ..., 199361 199362 199363]|Valid: [    0     1     2 ..., 39870 39871 39872]\n",
      "Index,Train: [     0      1      2 ..., 199361 199362 199363]|Valid: [39873 39874 39875 ..., 79743 79744 79745]\n",
      "Index,Train: [     0      1      2 ..., 199361 199362 199363]|Valid: [ 79746  79747  79748 ..., 119616 119617 119618]\n",
      "Index,Train: [     0      1      2 ..., 199361 199362 199363]|Valid: [119619 119620 119621 ..., 159489 159490 159491]\n",
      "Index,Train: [     0      1      2 ..., 159489 159490 159491]|Valid: [159492 159493 159494 ..., 199361 199362 199363]\n",
      "\n",
      "** Train+Validation Data **\n",
      "Percentage of normal transactions: 0.9983 (n=199016)\n",
      "Percentage of fraud transactions: 0.0017 (n=348)\n",
      "Total number of transactions in resampled data: 199364\n",
      "\n",
      "** Test Data **\n",
      "Percentage of normal transactions: 0.9983 (n=85299)\n",
      "Percentage of fraud transactions: 0.0017 (n=144)\n",
      "Total number of transactions in resampled data: 85443\n"
     ]
    }
   ],
   "source": [
    "# ORIGINAL DATA\n",
    "# - df\n",
    "# - x_data\n",
    "# - y_labels\n",
    "\n",
    "# 2) Prepare the learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "x_data = df.drop('Class',axis=1)\n",
    "y_labels = df['Class']\n",
    "y_names = ['Normal','Fraud']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_data,y_labels,test_size=0.3,random_state=101)\n",
    "\n",
    "print('Train+Validation:%s, Test:%s'%(X_train.shape,X_test.shape))\n",
    "\n",
    "# 2.a) Random Cross Validation (10-folds)\n",
    "k_3folds = KFold(n_splits=3,random_state=101)\n",
    "k_5folds = KFold(n_splits=5,random_state=101)\n",
    "k_10folds = KFold(n_splits=10,random_state=101)\n",
    "for train_indices, validation_indices in k_5folds.split(X_train):\n",
    "     print('Index,Train: %s|Valid: %s' % (train_indices, validation_indices))\n",
    "\n",
    "# 2.b) Stratified Cross Validation (10-folds) - distributed based on labels\n",
    "sk_3folds = StratifiedKFold(n_splits=3, random_state=101)\n",
    "sk_5folds = StratifiedKFold(n_splits=5, random_state=101)\n",
    "sk_10folds = StratifiedKFold(n_splits=10, random_state=101)\n",
    "\n",
    "# Showing ratio\n",
    "dfaux = y_train\n",
    "print(\"\")\n",
    "print(\"** Train+Validation Data **\")\n",
    "print(\"Percentage of normal transactions: %1.4f (n=%d)\"%(len(dfaux[dfaux == 0])/float(len(dfaux)),len(dfaux[dfaux == 0])))\n",
    "print(\"Percentage of fraud transactions: %1.4f (n=%d)\"%(len(dfaux[dfaux == 1])/float(len(dfaux)),len(dfaux[dfaux == 1])))\n",
    "print(\"Total number of transactions in resampled data: %d\"%(len(dfaux)))\n",
    "# Showing ratio\n",
    "dfaux = y_test\n",
    "print(\"\")\n",
    "print(\"** Test Data **\")\n",
    "print(\"Percentage of normal transactions: %1.4f (n=%d)\"%(len(dfaux[dfaux == 0])/float(len(dfaux)),len(dfaux[dfaux == 0])))\n",
    "print(\"Percentage of fraud transactions: %1.4f (n=%d)\"%(len(dfaux[dfaux == 1])/float(len(dfaux)),len(dfaux[dfaux == 1])))\n",
    "print(\"Total number of transactions in resampled data: %d\"%(len(dfaux)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Dealing with Imbalanced Data: Sampling Approach\n",
    "\n",
    "**OBS**: Here, we have to be careful to not use the test for the sampling purpose... otherwise, we will already use the class information. Thus, the correct thing to do is a Sampling on the training data solely, leaving the test data completely apart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Undersampling: create dataset with proportional train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of normal transactions: 0.50 (n=348)\n",
      "Percentage of fraud transactions: 0.50 (n=348)\n",
      "Total number of transactions in resampled data: 696\n"
     ]
    }
   ],
   "source": [
    "# Set data to undersample. The example used the whole data, but maybe this is not correct...\n",
    "#datax = pd.concat([x_data,y_labels],axis=1) # This would use the whole data including Test... (maybe it is OK)\n",
    "datax0 = pd.concat([x_data,y_labels],axis=1) # ORIGINAL DATA\n",
    "datax = pd.concat([X_train,y_train],axis=1) #\n",
    "\n",
    "# Number of data points in the minority class\n",
    "number_records_fraud = len(datax[datax.Class == 1])\n",
    "fraud_indices = np.array(datax[datax.Class == 1].index)\n",
    "\n",
    "# Picking the indices of the normal classes\n",
    "normal_indices = datax[datax.Class == 0].index\n",
    "\n",
    "# Out of the indices we picked, randomly select \"x\" number (number_records_fraud)\n",
    "random_normal_indices = np.random.choice(normal_indices, number_records_fraud, replace = False)\n",
    "random_normal_indices = np.array(random_normal_indices)\n",
    "\n",
    "# Appending the 2 indices\n",
    "under_sample_indices = np.concatenate([fraud_indices,random_normal_indices])\n",
    "\n",
    "# Under sample dataset\n",
    "under_sampled_data = datax0.iloc[under_sample_indices,:]\n",
    "under_sampled_data_shuffled = under_sampled_data.sample(frac=1) # shuffle data to use for train/test\n",
    "\n",
    "X_undersampled = under_sampled_data_shuffled.loc[:, under_sampled_data_shuffled.columns != 'Class'] # Do not use direct to train/test...\n",
    "y_undersampled = under_sampled_data_shuffled.loc[:, under_sampled_data_shuffled.columns == 'Class']\n",
    "# Convert to Series (JI)\n",
    "y_undersampled = y_undersampled.iloc[:,0]\n",
    "\n",
    "# Showing ratio\n",
    "aux0 = len(under_sampled_data[under_sampled_data.Class == 0])\n",
    "aux1 = len(under_sampled_data[under_sampled_data.Class == 1])\n",
    "print(\"Percentage of normal transactions: %1.2f (n=%d)\"%(aux0/float(len(under_sampled_data)),aux0))\n",
    "print(\"Percentage of fraud transactions: %1.2f (n=%d)\"%(aux1/float(len(under_sampled_data)),aux1))\n",
    "print(\"Total number of transactions in resampled data: %d\"%(len(under_sampled_data)))\n",
    "\n",
    "# # Split into Train and Validation data sets\n",
    "# #from sklearn.cross_validation import train_test_split\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Undersampled dataset\n",
    "# X_train_undersampled, X_test_undersampled, y_train_undersampled, y_test_undersampled = \\\n",
    "#                train_test_split(X_undersampled,y_undersampled,test_size = 0.3,random_state = 0)\n",
    "# print(\"\")\n",
    "# print(\"Number transactions train dataset: \", len(X_train_undersampled))\n",
    "# print(\"Number transactions validation dataset: \", len(X_test_undersampled))\n",
    "# print(\"Total number of transactions: \", len(X_train_undersampled)+len(X_test_undersampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) SMOTE: Synthetic Minority Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied SMOTE: y_train old_size=199364 (#fraud=348), new_size=398032 (#fraud=199016)\n",
      "Applied SMOTE (borderline): y_train old_size=199364 (#fraud=348), new_size=398032 (#fraud=199016)\n"
     ]
    }
   ],
   "source": [
    "# SMOTE (Ide, ICPR 2016)\n",
    "# Check: - ji_Example_Nice_ML_Framework.ipynb\n",
    "#        - ji_example_SMOTE.ipynb\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "# SMOTE and others..\n",
    "\n",
    "# a) Standard SMOTE\n",
    "sm = SMOTE(kind='regular',random_state=101)\n",
    "X_train_SMOTE, y_train_SMOTE = sm.fit_sample(X_train, y_train)\n",
    "print('Applied SMOTE: y_train old_size=%d (#fraud=%d), new_size=%d (#fraud=%d)'%(len(y_train),len(y_train[y_train==1]),len(y_train_SMOTE),len(y_train_SMOTE[y_train_SMOTE==1])))\n",
    "X_train_SMOTE = pd.DataFrame(X_train_SMOTE,columns=X_train.columns)\n",
    "y_train_SMOTE = pd.Series(y_train_SMOTE)\n",
    "# Shuffle data\n",
    "iprm = np.random.permutation(len(y_train_SMOTE))\n",
    "X_train_SMOTE = X_train_SMOTE.iloc[iprm]\n",
    "y_train_SMOTE = y_train_SMOTE.iloc[iprm]\n",
    "\n",
    "# b) Borderline SMOTE\n",
    "sm = SMOTE(kind='borderline2',random_state=101) # \n",
    "X_train_SMOTE2, y_train_SMOTE2 = sm.fit_sample(X_train, y_train)\n",
    "print('Applied SMOTE (borderline): y_train old_size=%d (#fraud=%d), new_size=%d (#fraud=%d)'%(len(y_train),len(y_train[y_train==1]),len(y_train_SMOTE2),len(y_train_SMOTE2[y_train_SMOTE2==1])))\n",
    "X_train_SMOTE2 = pd.DataFrame(X_train_SMOTE2,columns=X_train.columns)\n",
    "y_train_SMOTE2 = pd.Series(y_train_SMOTE2)\n",
    "# Shuffle data\n",
    "iprm = np.random.permutation(len(y_train_SMOTE2))\n",
    "X_train_SMOTE2 = X_train_SMOTE2.iloc[iprm]\n",
    "y_train_SMOTE2 = y_train_SMOTE2.iloc[iprm]\n",
    "\n",
    "#sm = SMOTE(kind='svm')\n",
    "# c) ADASYN\n",
    "#sm = ADASYN()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "Ref: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1) Check correlation among feats \n",
    "# --> you can consider removing correlated feats, since it creates colinearity in regression-based approaches\n",
    "\n",
    "# Cross-correlation map of the features\n",
    "f,ax = plt.subplots(figsize=(20, 20))\n",
    "#sns.heatmap(X_train.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['V1', 'V2', 'V3', 'V4', 'V5', 'V7', 'V9', 'V10', 'V11', 'V12', 'V14',\n",
      "       'V16', 'V17', 'V18', 'V21'],\n",
      "      dtype='object')\n",
      "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V7', 'V8', 'V10', 'V11', 'V12',\n",
      "       'V14', 'V16', 'V17', 'Amount'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# 2) Univariate feature selection\n",
    "# - In univariate feature selection, we will use SelectKBest that removes all but the k highest scoring features. \n",
    "# - http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest,chi2,f_classif,mutual_info_classif\n",
    "\n",
    "# Some options: f_classif (ANOVA), chi2, mutual_info_classif\n",
    "# find k-best scored 10 features\n",
    "select_feature1 = SelectKBest(f_classif, k=15).fit(X_train, y_train)\n",
    "print(X_train.columns[select_feature1.get_support()])\n",
    "select_feature2 = SelectKBest(chi2, k=15).fit(X_train.abs(), y_train)\n",
    "print(X_train.columns[select_feature2.get_support()])\n",
    "#select_feature3 = SelectKBest(mutual_info_classif, k=10).fit(X_train, y_train)\n",
    "#print(X_train.columns[select_feature3.get_support()])\n",
    "\n",
    "# Transform data\n",
    "X_train_UniF = select_feature1.transform(X_train)\n",
    "X_test_UniF = select_feature1.transform(X_test)\n",
    "X_train_Chi2 = select_feature2.transform(X_train)\n",
    "X_test_Chi2 = select_feature2.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen best 15 features by Recursive Feature Elimination: Index(['Time', 'V4', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V14', 'V16',\n",
      "       'V17', 'V20', 'V21', 'Amount'],\n",
      "      dtype='object')\n",
      "Computation time: 0 seconds\n"
     ]
    }
   ],
   "source": [
    "# 3) Recursive feature elimination (RFE) using Random ForestÂ¶ (could be any other...)\n",
    "# - http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html \n",
    "# - Basically, it uses one of the classification methods (random forest in our example), \n",
    "# assign weights to each of features. Whose absolute weights are the smallest are pruned \n",
    "# from the current set features. That procedure is recursively repeated on the pruned set until \n",
    "# the desired number of features\n",
    "tic = time.time()\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier # Random forest classifier\n",
    "\n",
    "# Here, we will apply RFE on the undersampled data so that it will not be affected by the data imbalance\n",
    "\n",
    "# Create the RFE object and rank each pixel\n",
    "clf_rf_3 = RandomForestClassifier(n_estimators=10)      \n",
    "rfe = RFE(estimator=clf_rf_3, n_features_to_select=15, step=1)\n",
    "# Normalize\n",
    "XX_train = X_undersampled.copy()\n",
    "yy_train = y_undersampled.copy()\n",
    "cols_to_norm = ['Time','Amount']\n",
    "#XX_train[cols_to_norm] = XX_train[cols_to_norm].apply(lambda x: (x - x.mean()) / x.std())\n",
    "XX_train[cols_to_norm] = XX_train[cols_to_norm].apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "\n",
    "rfe = rfe.fit(XX_train, yy_train)\n",
    "print('Chosen best 15 features by Recursive Feature Elimination:',XX_train.columns[rfe.support_])\n",
    "\n",
    "X_train_RFE = rfe.transform(X_train)\n",
    "X_test_RFE = rfe.transform(X_test)\n",
    "\n",
    "toc = time.time()\n",
    "print(\"Computation time: %d seconds\"%(toc-tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(696, 30)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4) t-SNE (good for visualization)\n",
    "# - t-Distributed Stochastic Neighbor Embedding (t-SNE) is a (prize-winning) technique for dimensionality reduction\n",
    "# that is particularly well suited for the visualization of high-dimensional datasets. The technique can be \n",
    "# implemented via Barnes-Hut approximations, allowing it to be applied on large real-world datasets. \n",
    "# - t-distributed Stochastic Neighbor Embedding. t-SNE [1] is a tool to visualize high-dimensional data. \n",
    "# It converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler \n",
    "# divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data.\n",
    "# t-SNE has a cost function that is not convex, i.e. with different initializations we can get different results.\n",
    "# - It is highly recommended to use another dimensionality reduction method (e.g. PCA for dense data or \n",
    "# TruncatedSVD for sparse data) to reduce the number of dimensions to a reasonable amount (e.g. 50) \n",
    "# if the number of features is very high. This will suppress some noise and speed up the computation of \n",
    "# pairwise distances between samples. For more tips see Laurens van der Maatenâs FAQ [2].\n",
    "# - Ref: - https://lvdmaaten.github.io/tsne/\n",
    "#        - http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.manifold import TSNE\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Select the dataset to transform\n",
    "XX_train = X_undersampled.copy()\n",
    "yy_train = y_undersampled.copy()\n",
    "\n",
    "#Scale features to improve the training ability of TSNE.\n",
    "standard_scaler = StandardScaler()\n",
    "XX_train_std = standard_scaler.fit_transform(XX_train)\n",
    "XX_train_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation time: 10 seconds\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "tsne = TSNE(n_components=2, random_state=101)\n",
    "XX_train_2d = tsne.fit_transform(XX_train_std)\n",
    "toc = time.time()\n",
    "print(\"Computation time: %d seconds\"%(toc-tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfIAAAFnCAYAAABdOssgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMSnkQgghhIr9D8r53Jj8UegvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6477ad58d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Build the scatter plot with the two types of transactions.\n",
    "color_map = {0:'red', 1:'blue'}\n",
    "#plt.figure()\n",
    "#yy = yy_train.Class.values\n",
    "yy = yy_train.values\n",
    "allX = XX_train_2d[:,0]\n",
    "allY = XX_train_2d[:,1]\n",
    "plt.scatter(x = allX[yy==0], y = allY[yy==0], c = 'blue', label = 'Normal')\n",
    "plt.scatter(x = allX[yy==1], y = allY[yy==1], c = 'red', label = 'Fraud')\n",
    "plt.xlabel('X in t-SNE')\n",
    "plt.ylabel('Y in t-SNE')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('t-SNE visualization of XX_train data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3) Stratification approach: GNG + COM\n",
    "\n",
    "Probably not relevant for this data which is not that small..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting methods\n",
    "- XGBoost [x]\n",
    "- LightGBM\n",
    "- CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Useful functions\n",
    "def get_auc(m, Xtrain,ytrain, Xtest,ytest): \n",
    "    return (metrics.roc_auc_score(ytrain,m.predict_proba(Xtrain)[:,1]),\n",
    "            metrics.roc_auc_score(ytest,m.predict_proba(Xtest)[:,1]))\n",
    "\n",
    "def get_metrics(model,XX,yy):    \n",
    "    cnf_matrix = confusion_matrix(yy,model.predict(XX))\n",
    "    # Get scores\n",
    "    res = ji_get_metrics(cnf_matrix)\n",
    "    print(\" Precision: %1.2f\"%(res.Precision))\n",
    "    print(\" Recall(TPR): %1.2f\"%(res.Recall))\n",
    "    print(\" BAcc=(TPR+TNR)/2: %1.2f\"%(res.BAcc))\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "# Install with conda:\n",
    "# - Check file to instal: anaconda search -t conda xgboost\n",
    "# - conda install -c mndrake xgboost (IF mndrake is listed...)\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn import metrics\n",
    "\n",
    "# Wraper function: Tune + Train + Test\n",
    "def run_XGBoost(XX_train,yy_train,XX_test,yy_test,explabel,flag):\n",
    "    tic = time.time()\n",
    "\n",
    "    # 1) Parameter Tuning    \n",
    "    model = xgb.XGBClassifier()\n",
    "    if flag.set2use==0:\n",
    "        param_dist = {\"max_depth\": [10,30,50], # 27 combinations\n",
    "                  \"min_child_weight\" : [1,3,6],\n",
    "                  \"n_estimators\": [200],\n",
    "                  \"learning_rate\": [0.01,0.05, 0.1]}\n",
    "    else: # 125 combinations\n",
    "        param_dist = {\"max_depth\": [10,30,50,70,90],\n",
    "                  \"min_child_weight\" : [1,3,6,9,12],\n",
    "                  \"n_estimators\": [200],\n",
    "                  \"learning_rate\": [0.01,0.05, 0.1,0.15,0.2],}\n",
    "\n",
    "    if flag.pars2use:\n",
    "        best = flag.pars2use\n",
    "        print('Using parameters:', best)\n",
    "    else:\n",
    "        # Grid search\n",
    "        grid_search = GridSearchCV(model, param_grid=param_dist, cv = 3,verbose=1, n_jobs=-1)\n",
    "        grid_search.fit(XX_train, yy_train)\n",
    "        toc = time.time()\n",
    "        print(\"Computation time (parameter tunning): %d seconds...\"%(toc-tic))   \n",
    "        best = grid_search.best_params_\n",
    "        print('Best parameters:', best)\n",
    "    \n",
    "    # 3) Run with best parameters\n",
    "    tic = time.time()\n",
    "    model = xgb.XGBClassifier(max_depth=best['max_depth'], min_child_weight=best['min_child_weight'], \\\n",
    "                              n_estimators=best['n_estimators'],n_jobs=-1 , verbose=1,learning_rate=best['learning_rate'])\n",
    "    model.fit(XX_train,yy_train)\n",
    "\n",
    "    # Evaluate\n",
    "    print('********** SUMMARY: XGBoost (%s) **************'%(explabel))\n",
    "    print('- Parameters used: ',best)\n",
    "    print('- AUC(Train|Test): ',get_auc(model,XX_train,yy_train,XX_test,yy_test))\n",
    "    print('- Metrics: Train data (n=%d) **'%len(yy_train))\n",
    "    res_train = get_metrics(model,XX_train,yy_train)\n",
    "    print('- Metrics: Test data (n=%d) **'%len(yy_test))\n",
    "    res_test = get_metrics(model,XX_test,yy_test)\n",
    "\n",
    "    toc = time.time()\n",
    "    print(\"Computation time (Total): %d seconds\"%(toc-tic))\n",
    "    \n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    5.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation time (parameter tunning): 5 seconds...\n",
      "Best parameters: {'learning_rate': 0.05, 'max_depth': 10, 'min_child_weight': 6, 'n_estimators': 200}\n",
      "********** SUMMARY: XGBoost (Exp1-Undersampled) **************\n",
      "- Parameters used:  {'learning_rate': 0.05, 'max_depth': 10, 'min_child_weight': 6, 'n_estimators': 200}\n",
      "- AUC(Train|Test):  (0.99980182322631783, 0.98394699169327249)\n",
      "- Metrics: Train data (n=696) **\n",
      " TN=348, FP=0\n",
      " FN=6, TP=342\n",
      " Precision: 1.00\n",
      " Recall(TPR): 0.98\n",
      " BAcc=(TPR+TNR)/2: 0.99\n",
      "- Metrics: Test data (n=85443) **\n",
      " TN=82434, FP=2865\n",
      " FN=15, TP=129\n",
      " Precision: 0.04\n",
      " Recall(TPR): 0.90\n",
      " BAcc=(TPR+TNR)/2: 0.93\n",
      "Computation time (Total): 0 seconds\n"
     ]
    }
   ],
   "source": [
    "## EXPERIMENT 1 - Train data: undersampled, Test data: all\n",
    "\n",
    "# 1) Choose dataset\n",
    "mylabel = 'Exp1-Undersampled'\n",
    "XX_train = X_undersampled.copy()\n",
    "yy_train = y_undersampled.copy()\n",
    "\n",
    "XX_test = X_test.copy()\n",
    "yy_test = y_test.copy()\n",
    "# 2) Run wrapper\n",
    "flag = matlab_like()\n",
    "flag.set2use = 1\n",
    "flag.pars2use = {}\n",
    "best1 = run_XGBoost(XX_train,yy_train,XX_test,yy_test,mylabel,flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    3.2s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    5.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation time (parameter tunning): 5 seconds...\n",
      "Best parameters: {'learning_rate': 0.05, 'max_depth': 10, 'min_child_weight': 6, 'n_estimators': 200}\n",
      "********** SUMMARY: XGBoost (Exp2-Undersampled (normalized)) **************\n",
      "- Parameters used:  {'learning_rate': 0.05, 'max_depth': 10, 'min_child_weight': 6, 'n_estimators': 200}\n",
      "- AUC(Train|Test):  (0.99980182322631783, 0.94721838767160227)\n",
      "- Metrics: Train data (n=696) **\n",
      " TN=348, FP=0\n",
      " FN=6, TP=342\n",
      " Precision: 1.00\n",
      " Recall(TPR): 0.98\n",
      " BAcc=(TPR+TNR)/2: 0.99\n",
      "- Metrics: Test data (n=85443) **\n",
      " TN=9371, FP=75928\n",
      " FN=1, TP=143\n",
      " Precision: 0.00\n",
      " Recall(TPR): 0.99\n",
      " BAcc=(TPR+TNR)/2: 0.55\n",
      "Computation time (Total): 0 seconds\n"
     ]
    }
   ],
   "source": [
    "## EXPERIMENT 2 - Train data: undersampled, Test data: all (normalized data)\n",
    "\n",
    "# 1) Choose dataset\n",
    "mylabel = 'Exp2-Undersampled (normalized)'\n",
    "XX_train = ji_normalize(X_undersampled,'standardize',[])\n",
    "yy_train = y_undersampled.copy()\n",
    "\n",
    "XX_test = ji_normalize(X_test,'standardize',[])\n",
    "yy_test = y_test.copy()\n",
    "# 2) Run wrapper\n",
    "flag = matlab_like()\n",
    "flag.set2use = 1\n",
    "flag.pars2use = {}\n",
    "best2 = run_XGBoost(XX_train,yy_train,XX_test,yy_test,mylabel,flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using parameters: {'learning_rate': 0.05, 'max_depth': 10, 'min_child_weight': 6, 'n_estimators': 200}\n",
      "********** SUMMARY: XGBoost (Exp3-SMOTE (best parameters:Exp1)) **************\n",
      "- Parameters used:  {'learning_rate': 0.05, 'max_depth': 10, 'min_child_weight': 6, 'n_estimators': 200}\n",
      "- AUC(Train|Test):  (0.99999814343887916, 0.98250410972643953)\n",
      "- Metrics: Train data (n=398032) **\n",
      " TN=198986, FP=30\n",
      " FN=0, TP=199016\n",
      " Precision: 1.00\n",
      " Recall(TPR): 1.00\n",
      " BAcc=(TPR+TNR)/2: 1.00\n",
      "- Metrics: Test data (n=85443) **\n",
      " TN=85266, FP=33\n",
      " FN=20, TP=124\n",
      " Precision: 0.79\n",
      " Recall(TPR): 0.86\n",
      " BAcc=(TPR+TNR)/2: 0.93\n",
      "Computation time (Total): 87 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.05,\n",
       " 'max_depth': 10,\n",
       " 'min_child_weight': 6,\n",
       " 'n_estimators': 200}"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## EXPERIMENT 3 - Train data: SMOTE, Test data: all (best parameters from Exp1)\n",
    "\n",
    "# 1) Choose dataset\n",
    "mylabel = 'Exp3-SMOTE (best parameters:Exp1)'\n",
    "XX_train = X_train_SMOTE.copy()\n",
    "yy_train = y_train_SMOTE.copy()\n",
    "XX_test = X_test.copy()\n",
    "yy_test = y_test.copy()\n",
    "# 2) Run wrapper\n",
    "flag = matlab_like()\n",
    "flag.set2use = 0 # smaller set\n",
    "flag.pars2use = best1 # will not run GridSearch if selected\n",
    "run_XGBoost(XX_train,yy_train,XX_test,yy_test,mylabel,flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using parameters: {'learning_rate': 0.05, 'max_depth': 10, 'min_child_weight': 6, 'n_estimators': 200}\n",
      "********** SUMMARY: XGBoost (Exp4-SMOTE2 (best parameters:Exp1)) **************\n",
      "- Parameters used:  {'learning_rate': 0.05, 'max_depth': 10, 'min_child_weight': 6, 'n_estimators': 200}\n",
      "- AUC(Train|Test):  (0.99999833710233976, 0.98295973738131626)\n",
      "- Metrics: Train data (n=398032) **\n",
      " TN=198996, FP=20\n",
      " FN=35, TP=198981\n",
      " Precision: 1.00\n",
      " Recall(TPR): 1.00\n",
      " BAcc=(TPR+TNR)/2: 1.00\n",
      "- Metrics: Test data (n=85443) **\n",
      " TN=85275, FP=24\n",
      " FN=23, TP=121\n",
      " Precision: 0.83\n",
      " Recall(TPR): 0.84\n",
      " BAcc=(TPR+TNR)/2: 0.92\n",
      "Computation time (Total): 76 seconds\n"
     ]
    }
   ],
   "source": [
    "## EXPERIMENT 4 - Train data: SMOTE2, Test data: all (best parameters from Exp1)\n",
    "\n",
    "# 1) Choose dataset\n",
    "mylabel = 'Exp4-SMOTE2 (best parameters:Exp1)'\n",
    "XX_train = X_train_SMOTE2.copy()\n",
    "yy_train = y_train_SMOTE2.copy()\n",
    "XX_test = X_test.copy()\n",
    "yy_test = y_test.copy()\n",
    "# 2) Run wrapper\n",
    "flag = matlab_like()\n",
    "flag.set2use = 0 # smaller set\n",
    "flag.pars2use = best1 # will not run GridSearch if selected\n",
    "tmp = run_XGBoost(XX_train,yy_train,XX_test,yy_test,mylabel,flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 304 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 344 out of 375 | elapsed:    3.0s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    3.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation time (parameter tunning): 3 seconds...\n",
      "Best parameters: {'learning_rate': 0.01, 'max_depth': 10, 'min_child_weight': 6, 'n_estimators': 200}\n",
      "********** SUMMARY: XGBoost (Exp5-Undersampled) **************\n",
      "- Parameters used:  {'learning_rate': 0.01, 'max_depth': 10, 'min_child_weight': 6, 'n_estimators': 200}\n",
      "- AUC(Train|Test):  (0.99272526093275193, 0.9776254378389222)\n",
      "- Metrics: Train data (n=696) **\n",
      " TN=339, FP=9\n",
      " FN=26, TP=322\n",
      " Precision: 0.97\n",
      " Recall(TPR): 0.93\n",
      " BAcc=(TPR+TNR)/2: 0.95\n",
      "- Metrics: Test data (n=85443) **\n",
      " TN=82646, FP=2653\n",
      " FN=17, TP=127\n",
      " Precision: 0.05\n",
      " Recall(TPR): 0.88\n",
      " BAcc=(TPR+TNR)/2: 0.93\n",
      "Computation time (Total): 0 seconds\n",
      "Using parameters: {'learning_rate': 0.01, 'max_depth': 10, 'min_child_weight': 6, 'n_estimators': 200}\n",
      "********** SUMMARY: XGBoost (Exp5-SMOTE (best parameters from Undersampled)) **************\n",
      "- Parameters used:  {'learning_rate': 0.01, 'max_depth': 10, 'min_child_weight': 6, 'n_estimators': 200}\n",
      "- AUC(Train|Test):  (0.99986035084522029, 0.97706055398591363)\n",
      "- Metrics: Train data (n=398032) **\n",
      " TN=198679, FP=337\n",
      " FN=687, TP=198329\n",
      " Precision: 1.00\n",
      " Recall(TPR): 1.00\n",
      " BAcc=(TPR+TNR)/2: 1.00\n",
      "- Metrics: Test data (n=85443) **\n",
      " TN=85094, FP=205\n",
      " FN=21, TP=123\n",
      " Precision: 0.38\n",
      " Recall(TPR): 0.85\n",
      " BAcc=(TPR+TNR)/2: 0.93\n",
      "Computation time (Total): 65 seconds\n"
     ]
    }
   ],
   "source": [
    "## EXPERIMENT 5 - Train data: SMOTE + Selected Feats, Test data: all (best parameters from Undersampled data)\n",
    "\n",
    "# 1) Choose dataset\n",
    "mylabel = 'Exp5-SMOTE (best parameters from Undersampled)'\n",
    "iselected = X_train.columns[select_feature1.get_support()] # Univariate ANOVA\n",
    "XX_train = X_train_SMOTE[iselected].copy()\n",
    "yy_train = y_train_SMOTE.copy()\n",
    "XX_test = X_test[iselected].copy()\n",
    "yy_test = y_test.copy()\n",
    "# 2) Get best parameters from Undersampled data\n",
    "flag = matlab_like()\n",
    "flag.set2use = 1 # smaller set\n",
    "flag.pars2use = {} # will not run GridSearch if selected\n",
    "best5 = run_XGBoost(X_undersampled[iselected],y_undersampled,XX_test,yy_test,'Exp5-Undersampled',flag)\n",
    "# 3) Run on whole data with the best parameters from Undersampled data\n",
    "flag = matlab_like()\n",
    "flag.set2use = 0 # smaller set\n",
    "flag.pars2use = best5 # will not run GridSearch if selected\n",
    "best5 = run_XGBoost(XX_train,yy_train,XX_test,yy_test,mylabel,flag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    3.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation time (parameter tunning): 3 seconds...\n",
      "Best parameters: {'learning_rate': 0.01, 'max_depth': 10, 'min_child_weight': 6, 'n_estimators': 200}\n",
      "********** SUMMARY: XGBoost (Exp6-Undersampled) **************\n",
      "- Parameters used:  {'learning_rate': 0.01, 'max_depth': 10, 'min_child_weight': 6, 'n_estimators': 200}\n",
      "- AUC(Train|Test):  (0.99272526093275193, 0.9776254378389222)\n",
      "- Metrics: Train data (n=696) **\n",
      " TN=339, FP=9\n",
      " FN=26, TP=322\n",
      " Precision: 0.97\n",
      " Recall(TPR): 0.93\n",
      " BAcc=(TPR+TNR)/2: 0.95\n",
      "- Metrics: Test data (n=85443) **\n",
      " TN=82646, FP=2653\n",
      " FN=17, TP=127\n",
      " Precision: 0.05\n",
      " Recall(TPR): 0.88\n",
      " BAcc=(TPR+TNR)/2: 0.93\n",
      "Computation time (Total): 0 seconds\n",
      "Using parameters: {'learning_rate': 0.01, 'max_depth': 10, 'min_child_weight': 6, 'n_estimators': 200}\n",
      "********** SUMMARY: XGBoost (Exp6-SMOTE2 (best parameters from Undersampled)) **************\n",
      "- Parameters used:  {'learning_rate': 0.01, 'max_depth': 10, 'min_child_weight': 6, 'n_estimators': 200}\n",
      "- AUC(Train|Test):  (0.99986703870436522, 0.95360087912975411)\n",
      "- Metrics: Train data (n=398032) **\n",
      " TN=198901, FP=115\n",
      " FN=92, TP=198924\n",
      " Precision: 1.00\n",
      " Recall(TPR): 1.00\n",
      " BAcc=(TPR+TNR)/2: 1.00\n",
      "- Metrics: Test data (n=85443) **\n",
      " TN=85223, FP=76\n",
      " FN=25, TP=119\n",
      " Precision: 0.61\n",
      " Recall(TPR): 0.83\n",
      " BAcc=(TPR+TNR)/2: 0.91\n",
      "Computation time (Total): 60 seconds\n"
     ]
    }
   ],
   "source": [
    "## EXPERIMENT 6 - Train data: SMOTE2 + Selected Feats, Test data: all (best parameters from Undersampled data)\n",
    "\n",
    "# 1) Choose dataset\n",
    "mylabel = 'Exp6-SMOTE2 (best parameters from Undersampled)'\n",
    "iselected = X_train.columns[select_feature1.get_support()] # Univariate ANOVA\n",
    "XX_train = X_train_SMOTE2[iselected].copy()\n",
    "yy_train = y_train_SMOTE2.copy()\n",
    "XX_test = X_test[iselected].copy()\n",
    "yy_test = y_test.copy()\n",
    "# 2) Get best parameters from Undersampled data\n",
    "flag = matlab_like()\n",
    "flag.set2use = 1 # smaller set\n",
    "flag.pars2use = {} # will not run GridSearch if selected\n",
    "best6 = run_XGBoost(X_undersampled[iselected],y_undersampled,XX_test,yy_test,'Exp6-Undersampled',flag)\n",
    "# 3) Run on whole data with the best parameters from Undersampled data\n",
    "flag = matlab_like()\n",
    "flag.set2use = 0 # smaller set\n",
    "flag.pars2use = best6 # will not run GridSearch if selected\n",
    "tmp = run_XGBoost(XX_train,yy_train,XX_test,yy_test,mylabel,flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected feats:  Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V7', 'V8', 'V10', 'V11', 'V12',\n",
      "       'V14', 'V16', 'V17', 'Amount'],\n",
      "      dtype='object')\n",
      "Fitting 3 folds for each of 125 candidates, totalling 375 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done 304 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 344 out of 375 | elapsed:    3.0s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 375 out of 375 | elapsed:    3.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation time (parameter tunning): 4 seconds...\n",
      "Best parameters: {'learning_rate': 0.01, 'max_depth': 10, 'min_child_weight': 1, 'n_estimators': 200}\n",
      "********** SUMMARY: XGBoost (Exp7-Undersampled) **************\n",
      "- Parameters used:  {'learning_rate': 0.01, 'max_depth': 10, 'min_child_weight': 1, 'n_estimators': 200}\n",
      "- AUC(Train|Test):  (0.99997522790328974, 0.97433619125403315)\n",
      "- Metrics: Train data (n=696) **\n",
      " TN=348, FP=0\n",
      " FN=4, TP=344\n",
      " Precision: 1.00\n",
      " Recall(TPR): 0.99\n",
      " BAcc=(TPR+TNR)/2: 0.99\n",
      "- Metrics: Test data (n=85443) **\n",
      " TN=80522, FP=4777\n",
      " FN=13, TP=131\n",
      " Precision: 0.03\n",
      " Recall(TPR): 0.91\n",
      " BAcc=(TPR+TNR)/2: 0.93\n",
      "Computation time (Total): 0 seconds\n",
      "Using parameters: {'learning_rate': 0.01, 'max_depth': 10, 'min_child_weight': 1, 'n_estimators': 200}\n",
      "********** SUMMARY: XGBoost (Exp7-SMOTE2 (best parameters from Undersampled)) **************\n",
      "- Parameters used:  {'learning_rate': 0.01, 'max_depth': 10, 'min_child_weight': 1, 'n_estimators': 200}\n",
      "- AUC(Train|Test):  (0.99986347039103807, 0.96300350661919953)\n",
      "- Metrics: Train data (n=398032) **\n",
      " TN=198934, FP=82\n",
      " FN=47, TP=198969\n",
      " Precision: 1.00\n",
      " Recall(TPR): 1.00\n",
      " BAcc=(TPR+TNR)/2: 1.00\n",
      "- Metrics: Test data (n=85443) **\n",
      " TN=85238, FP=61\n",
      " FN=26, TP=118\n",
      " Precision: 0.66\n",
      " Recall(TPR): 0.82\n",
      " BAcc=(TPR+TNR)/2: 0.91\n",
      "Computation time (Total): 57 seconds\n"
     ]
    }
   ],
   "source": [
    "## EXPERIMENT 7 - Train data: SMOTE2 + Selected Feats (Chi2), Test data: all (best parameters from Undersampled data)\n",
    "\n",
    "# 1) Choose dataset\n",
    "mylabel = 'Exp7-SMOTE2 (best parameters from Undersampled)'\n",
    "iselected = X_train.columns[select_feature2.get_support()] # Chi2\n",
    "print('Selected feats: ',iselected)\n",
    "XX_train = X_train_SMOTE2[iselected].copy()\n",
    "yy_train = y_train_SMOTE2.copy()\n",
    "XX_test = X_test[iselected].copy()\n",
    "yy_test = y_test.copy()\n",
    "# 2) Get best parameters from Undersampled data\n",
    "flag = matlab_like()\n",
    "flag.set2use = 1 # smaller set\n",
    "flag.pars2use = {} # will not run GridSearch if selected\n",
    "tmp = run_XGBoost(X_undersampled[iselected],y_undersampled,XX_test,yy_test,'Exp7-Undersampled',flag)\n",
    "# 3) Run on whole data with the best parameters from Undersampled data\n",
    "flag = matlab_like()\n",
    "flag.set2use = 0 # smaller set\n",
    "flag.pars2use = tmp # will not run GridSearch if selected\n",
    "best7 = run_XGBoost(XX_train,yy_train,XX_test,yy_test,mylabel,flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
