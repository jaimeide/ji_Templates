{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch \n",
    "\n",
    "- Better compatibility with Python libraries such as Scikit-learn, numpy\n",
    "- More Pythonian\n",
    "- Used by Facebook, Tweeter and NVIDIA\n",
    "- Computational graphs are at the core of DL packages and allow efficient parallel processing\n",
    "\n",
    "Ref:http://adventuresinmachinelearning.com/pytorch-tutorial-deep-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics: Tensor, Variable, Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.,  3.,  3.],\n",
       "        [ 3.,  3.,  3.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TENSORS: are basically the Arrays in numpy\n",
    "x = torch.ones(2,3)\n",
    "y = torch.ones(2,3) * 2\n",
    "z = x+y\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# AUTOGRAD --> refer to the gradient of the backpropagation\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Create a variable that has gradient (i.e. updated during backprop)\n",
    "x = Variable(torch.ones(2, 2), requires_grad=True)\n",
    "# Variable containing:\n",
    "# 1  1\n",
    "# 1  1\n",
    "# [torch.FloatTensor of size 2x2]\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.,  5.],\n",
      "        [ 5.,  5.]])\n"
     ]
    }
   ],
   "source": [
    "# Suppose that y = f(x), say\n",
    "y = 2*x*x + 3*x # dy/dx=4x+3\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 7.,  7.],\n",
      "        [ 7.,  7.]])\n"
     ]
    }
   ],
   "source": [
    "# We can backprop with:\n",
    "y.backward(torch.ones(2, 2))\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-a7c78a1a530c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# If you backprop again, it will keep adding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cvision\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \"\"\"\n\u001b[1;32m---> 93\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cvision\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "# The backprop can not be repeated... because hte buffer is freed after calculation!\n",
    "y.backward(torch.ones(2, 2))\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.,  5.],\n",
      "        [ 5.,  5.]])\n",
      "tensor([[ 14.,  14.],\n",
      "        [ 14.,  14.]])\n"
     ]
    }
   ],
   "source": [
    "# To do backprop several time, use retain_graph\n",
    "y = 2*x*x + 3*x # dy/dx=4x+3\n",
    "print(y)\n",
    "y.backward(torch.ones(2, 2), retain_graph=True)\n",
    "# the retain_variables flag will prevent the internal buffers from being freed\n",
    "print(x.grad) # 7+7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.,  2.],\n",
      "        [ 2.,  2.]])\n",
      "tensor([[ 12.,  12.],\n",
      "        [ 12.,  12.]])\n",
      "tensor([[ 24.,  24.],\n",
      "        [ 24.,  24.]])\n"
     ]
    }
   ],
   "source": [
    "# Backprop done in a chain\n",
    "x = Variable(torch.ones(2,2),requires_grad=True)\n",
    "y = 2*x # \n",
    "print(y)\n",
    "z = y * y * 3 # z=12x**2 --> dz/dx=24x \n",
    "#out = z.mean()\n",
    "#print(z, out)\n",
    "\n",
    "#out.backward()\n",
    "z.backward(torch.ones(2,2))\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.,  2.],\n",
      "        [ 2.,  2.]])\n",
      "tensor([[ 12.,  12.],\n",
      "        [ 12.,  12.]]) tensor(12.)\n",
      "Notice that out is the mean of z. When 24 is propagated back, it goes to 4 leaves\n",
      "x --> y --> z\n",
      "x --> y --> z |-->out\n",
      "x --> y --> z\n",
      "x --> y --> z\n",
      "tensor([[ 6.,  6.],\n",
      "        [ 6.,  6.]])\n"
     ]
    }
   ],
   "source": [
    "# Backprop done in a chain. But here, the gradient is split through the graph!!\n",
    "x = Variable(torch.ones(2,2),requires_grad=True)\n",
    "y = 2*x # \n",
    "print(y)\n",
    "z = y * y * 3 # z=12x**2 --> dz/dx=24x \n",
    "\n",
    "out = z.mean()\n",
    "print(z, out)\n",
    "print('Notice that out is the mean of z. When 24 is propagated back, it goes to 4 leaves')\n",
    "print('x --> y --> z')\n",
    "print('x --> y --> z |-->out')\n",
    "print('x --> y --> z')\n",
    "print('x --> y --> z')\n",
    "\n",
    "out.backward()\n",
    "print(x.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN: fully connected neural net\n",
    "\n",
    "Example with the MNIST dataset (28x28=724). Neural net with 4 layers as follows:\n",
    "\n",
    "728 --> 200 --> 200 --> 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F # activation functions\n",
    "import torch.optim as optim\n",
    "\n",
    "# 1) Define the Neural Net\n",
    "\n",
    "class Net(nn.Module):  # Inherited from class nn.Module\n",
    "    # Architecture\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__() # we need this to init the inherited class nn\n",
    "        self.fc1 = nn.Linear(28*28, 200)\n",
    "        self.fc2 = nn.Linear(200, 200)\n",
    "        self.fc3 = nn.Linear(200, 10)\n",
    "    \n",
    "    # Define the activation functions and set the flow\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        out = F.log_softmax(self.fc3(x))\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=200, bias=True)\n",
      "  (fc2): Linear(in_features=200, out_features=200, bias=True)\n",
      "  (fc3): Linear(in_features=200, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 2) Create an instance\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3) Define Optimizer\n",
    "learning_rate = 0.001\n",
    "mom = 0.9 # weight given to previous weight (concept: inertia)\n",
    "optimizer = optim.SGD(net.parameters(),lr=learning_rate, momentum=mom)\n",
    "#optimizer = optim.Adam(params=net.parameters(), lr=learning_rate)\n",
    "#optimizer = optim.RMSprop(params=net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4) Define Loss\n",
    "criterion = nn.NLLLoss() # Cross-entropy = Negative LogLik + Log_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 5) Define the datasets\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "spath = 'D:\\\\Dropbox (LCN)\\\\jisoft_LARGE\\\\0_data_MNIST'\n",
    "\n",
    "def get_train_loader(batch_size):\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(spath, train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=batch_size, shuffle=True)\n",
    "    return(train_loader)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(spath, train=False, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])),\n",
    "    batch_size=4, shuffle=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaime\\Anaconda3\\envs\\cvision\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Users\\jaime\\Anaconda3\\envs\\cvision\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 0.014088\n",
      "Train Epoch: 0 [1280/60000 (2%)]\tLoss: 0.051743\n",
      "Train Epoch: 0 [2560/60000 (4%)]\tLoss: 0.104228\n",
      "Train Epoch: 0 [3840/60000 (6%)]\tLoss: 0.015322\n",
      "Train Epoch: 0 [5120/60000 (9%)]\tLoss: 0.172192\n",
      "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.176680\n",
      "Train Epoch: 0 [7680/60000 (13%)]\tLoss: 0.035919\n",
      "Train Epoch: 0 [8960/60000 (15%)]\tLoss: 0.002589\n",
      "Train Epoch: 0 [10240/60000 (17%)]\tLoss: 0.209712\n",
      "Train Epoch: 0 [11520/60000 (19%)]\tLoss: 0.046042\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.014328\n",
      "Train Epoch: 0 [14080/60000 (23%)]\tLoss: 0.540322\n",
      "Train Epoch: 0 [15360/60000 (26%)]\tLoss: 0.056537\n",
      "Train Epoch: 0 [16640/60000 (28%)]\tLoss: 0.017432\n",
      "Train Epoch: 0 [17920/60000 (30%)]\tLoss: 0.131928\n",
      "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.050315\n",
      "Train Epoch: 0 [20480/60000 (34%)]\tLoss: 0.020682\n",
      "Train Epoch: 0 [21760/60000 (36%)]\tLoss: 0.091898\n",
      "Train Epoch: 0 [23040/60000 (38%)]\tLoss: 0.026878\n",
      "Train Epoch: 0 [24320/60000 (41%)]\tLoss: 0.003884\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.232299\n",
      "Train Epoch: 0 [26880/60000 (45%)]\tLoss: 0.024572\n",
      "Train Epoch: 0 [28160/60000 (47%)]\tLoss: 0.116879\n",
      "Train Epoch: 0 [29440/60000 (49%)]\tLoss: 0.067777\n",
      "Train Epoch: 0 [30720/60000 (51%)]\tLoss: 0.031162\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.023087\n",
      "Train Epoch: 0 [33280/60000 (55%)]\tLoss: 0.104812\n",
      "Train Epoch: 0 [34560/60000 (58%)]\tLoss: 0.016359\n",
      "Train Epoch: 0 [35840/60000 (60%)]\tLoss: 0.083187\n",
      "Train Epoch: 0 [37120/60000 (62%)]\tLoss: 0.152936\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.008246\n",
      "Train Epoch: 0 [39680/60000 (66%)]\tLoss: 0.010908\n",
      "Train Epoch: 0 [40960/60000 (68%)]\tLoss: 0.044450\n",
      "Train Epoch: 0 [42240/60000 (70%)]\tLoss: 0.021336\n",
      "Train Epoch: 0 [43520/60000 (73%)]\tLoss: 0.044065\n",
      "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.019814\n",
      "Train Epoch: 0 [46080/60000 (77%)]\tLoss: 0.015194\n",
      "Train Epoch: 0 [47360/60000 (79%)]\tLoss: 0.007549\n",
      "Train Epoch: 0 [48640/60000 (81%)]\tLoss: 0.040405\n",
      "Train Epoch: 0 [49920/60000 (83%)]\tLoss: 0.076960\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.051645\n",
      "Train Epoch: 0 [52480/60000 (87%)]\tLoss: 0.073924\n",
      "Train Epoch: 0 [53760/60000 (90%)]\tLoss: 0.029476\n",
      "Train Epoch: 0 [55040/60000 (92%)]\tLoss: 0.007566\n",
      "Train Epoch: 0 [56320/60000 (94%)]\tLoss: 0.048544\n",
      "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.034993\n",
      "Train Epoch: 0 [58880/60000 (98%)]\tLoss: 0.033897\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.018773\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 0.025784\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 0.044599\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 0.044171\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 0.031547\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.039141\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 0.012663\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 0.006465\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 0.017672\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 0.009355\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.018876\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.067680\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.014294\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.005316\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.005526\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.013061\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.004006\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.005235\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.038523\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.074845\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.009425\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.016840\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.030865\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.019547\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.010645\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.042467\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.013954\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.025290\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.014415\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.042135\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.085038\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.012400\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.047382\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.038363\n",
      "Train Epoch: 1 [43520/60000 (73%)]\tLoss: 0.036472\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.053572\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.009359\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.034956\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.061646\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.071813\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.052780\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.039511\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.030449\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.075189\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.023854\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.005057\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.068264\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.013620\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.079264\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.011791\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.007169\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.010713\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.029207\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.009479\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.085367\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.004091\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.004783\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.016163\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.026968\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.041103\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.047626\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.038488\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.002797\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.015146\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.006065\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.007519\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.150078\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.013390\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.007997\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.014838\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.023846\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.050335\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.004684\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.009529\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.029060\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.029758\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.001632\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.002923\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.157430\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.012876\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.006416\n",
      "Train Epoch: 2 [43520/60000 (73%)]\tLoss: 0.043793\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.124752\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.015262\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.021906\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.008765\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.018575\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.019832\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.010795\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.181862\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.007305\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.139583\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.011677\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.031272\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.004558\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.049259\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.003717\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.015378\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.022743\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.003464\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.018841\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.005810\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.027270\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.014519\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.082294\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.009786\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.028567\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.024499\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.006266\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.004730\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.015710\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.014157\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.085921\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.020559\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.005784\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.011077\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.026974\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.005956\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.023080\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.026140\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.021160\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.012776\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.064076\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.007743\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.003139\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.005614\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.019218\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.040327\n",
      "Train Epoch: 3 [43520/60000 (73%)]\tLoss: 0.004561\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.102489\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.018217\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.010849\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.002157\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.043923\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.050115\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.359361\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.041372\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.018182\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.002306\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.010232\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.006438\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.006790\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.013938\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.012379\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.027155\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.008279\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.012743\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.075072\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.024996\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.050029\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.006159\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.036556\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.001521\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.009512\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.002765\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.007287\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.006157\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.032484\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.011109\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.029434\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.030503\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.022485\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.009683\n",
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.096939\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.092457\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.020027\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.026250\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.025201\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.042711\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.016707\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.031473\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.011272\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.032110\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.011495\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.011963\n",
      "Train Epoch: 4 [43520/60000 (73%)]\tLoss: 0.019914\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.105758\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.013468\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.024746\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.077286\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.002308\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.035351\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.009280\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.007056\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.014640\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.095607\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.022419\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.007312\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.011161\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.043537\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.002746\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.014146\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.001847\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.004031\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.003323\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.035161\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.005037\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.040137\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.011387\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.017243\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.106599\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.008345\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.075935\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.010170\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.007718\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.149945\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.019446\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.014901\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.008061\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.017520\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.008633\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.011987\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.024608\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.029747\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.046637\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.005924\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.019770\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.009324\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.076708\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.028158\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.001526\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.019317\n",
      "Train Epoch: 5 [43520/60000 (73%)]\tLoss: 0.008104\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.005602\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.004082\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.009533\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.002989\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.005633\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.012255\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.008575\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.017524\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.003251\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.009934\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.015081\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.022610\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.052773\n",
      "Train Epoch: 6 [1280/60000 (2%)]\tLoss: 0.006671\n",
      "Train Epoch: 6 [2560/60000 (4%)]\tLoss: 0.002566\n",
      "Train Epoch: 6 [3840/60000 (6%)]\tLoss: 0.008839\n",
      "Train Epoch: 6 [5120/60000 (9%)]\tLoss: 0.030602\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.004836\n",
      "Train Epoch: 6 [7680/60000 (13%)]\tLoss: 0.006669\n",
      "Train Epoch: 6 [8960/60000 (15%)]\tLoss: 0.008717\n",
      "Train Epoch: 6 [10240/60000 (17%)]\tLoss: 0.064719\n",
      "Train Epoch: 6 [11520/60000 (19%)]\tLoss: 0.048391\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.024342\n",
      "Train Epoch: 6 [14080/60000 (23%)]\tLoss: 0.020312\n",
      "Train Epoch: 6 [15360/60000 (26%)]\tLoss: 0.012562\n",
      "Train Epoch: 6 [16640/60000 (28%)]\tLoss: 0.041156\n",
      "Train Epoch: 6 [17920/60000 (30%)]\tLoss: 0.007080\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.004612\n",
      "Train Epoch: 6 [20480/60000 (34%)]\tLoss: 0.037257\n",
      "Train Epoch: 6 [21760/60000 (36%)]\tLoss: 0.002452\n",
      "Train Epoch: 6 [23040/60000 (38%)]\tLoss: 0.002709\n",
      "Train Epoch: 6 [24320/60000 (41%)]\tLoss: 0.005624\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.009885\n",
      "Train Epoch: 6 [26880/60000 (45%)]\tLoss: 0.084452\n",
      "Train Epoch: 6 [28160/60000 (47%)]\tLoss: 0.005084\n",
      "Train Epoch: 6 [29440/60000 (49%)]\tLoss: 0.015837\n",
      "Train Epoch: 6 [30720/60000 (51%)]\tLoss: 0.009472\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.015912\n",
      "Train Epoch: 6 [33280/60000 (55%)]\tLoss: 0.001975\n",
      "Train Epoch: 6 [34560/60000 (58%)]\tLoss: 0.007993\n",
      "Train Epoch: 6 [35840/60000 (60%)]\tLoss: 0.183734\n",
      "Train Epoch: 6 [37120/60000 (62%)]\tLoss: 0.014285\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.011788\n",
      "Train Epoch: 6 [39680/60000 (66%)]\tLoss: 0.158095\n",
      "Train Epoch: 6 [40960/60000 (68%)]\tLoss: 0.021790\n",
      "Train Epoch: 6 [42240/60000 (70%)]\tLoss: 0.084367\n",
      "Train Epoch: 6 [43520/60000 (73%)]\tLoss: 0.025048\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.020277\n",
      "Train Epoch: 6 [46080/60000 (77%)]\tLoss: 0.010794\n",
      "Train Epoch: 6 [47360/60000 (79%)]\tLoss: 0.042898\n",
      "Train Epoch: 6 [48640/60000 (81%)]\tLoss: 0.004337\n",
      "Train Epoch: 6 [49920/60000 (83%)]\tLoss: 0.007884\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.005553\n",
      "Train Epoch: 6 [52480/60000 (87%)]\tLoss: 0.011893\n",
      "Train Epoch: 6 [53760/60000 (90%)]\tLoss: 0.031454\n",
      "Train Epoch: 6 [55040/60000 (92%)]\tLoss: 0.016439\n",
      "Train Epoch: 6 [56320/60000 (94%)]\tLoss: 0.130583\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.015581\n",
      "Train Epoch: 6 [58880/60000 (98%)]\tLoss: 0.011791\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.028367\n",
      "Train Epoch: 7 [1280/60000 (2%)]\tLoss: 0.015345\n",
      "Train Epoch: 7 [2560/60000 (4%)]\tLoss: 0.047798\n",
      "Train Epoch: 7 [3840/60000 (6%)]\tLoss: 0.016430\n",
      "Train Epoch: 7 [5120/60000 (9%)]\tLoss: 0.015702\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.014600\n",
      "Train Epoch: 7 [7680/60000 (13%)]\tLoss: 0.014271\n",
      "Train Epoch: 7 [8960/60000 (15%)]\tLoss: 0.011387\n",
      "Train Epoch: 7 [10240/60000 (17%)]\tLoss: 0.017126\n",
      "Train Epoch: 7 [11520/60000 (19%)]\tLoss: 0.014234\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.004756\n",
      "Train Epoch: 7 [14080/60000 (23%)]\tLoss: 0.009917\n",
      "Train Epoch: 7 [15360/60000 (26%)]\tLoss: 0.009299\n",
      "Train Epoch: 7 [16640/60000 (28%)]\tLoss: 0.009964\n",
      "Train Epoch: 7 [17920/60000 (30%)]\tLoss: 0.316519\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.004379\n",
      "Train Epoch: 7 [20480/60000 (34%)]\tLoss: 0.004622\n",
      "Train Epoch: 7 [21760/60000 (36%)]\tLoss: 0.017104\n",
      "Train Epoch: 7 [23040/60000 (38%)]\tLoss: 0.004392\n",
      "Train Epoch: 7 [24320/60000 (41%)]\tLoss: 0.025020\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.003948\n",
      "Train Epoch: 7 [26880/60000 (45%)]\tLoss: 0.015009\n",
      "Train Epoch: 7 [28160/60000 (47%)]\tLoss: 0.003306\n",
      "Train Epoch: 7 [29440/60000 (49%)]\tLoss: 0.059294\n",
      "Train Epoch: 7 [30720/60000 (51%)]\tLoss: 0.005116\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.002690\n",
      "Train Epoch: 7 [33280/60000 (55%)]\tLoss: 0.022996\n",
      "Train Epoch: 7 [34560/60000 (58%)]\tLoss: 0.007108\n",
      "Train Epoch: 7 [35840/60000 (60%)]\tLoss: 0.031712\n",
      "Train Epoch: 7 [37120/60000 (62%)]\tLoss: 0.032051\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.020461\n",
      "Train Epoch: 7 [39680/60000 (66%)]\tLoss: 0.029931\n",
      "Train Epoch: 7 [40960/60000 (68%)]\tLoss: 0.045544\n",
      "Train Epoch: 7 [42240/60000 (70%)]\tLoss: 0.017535\n",
      "Train Epoch: 7 [43520/60000 (73%)]\tLoss: 0.042554\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.007364\n",
      "Train Epoch: 7 [46080/60000 (77%)]\tLoss: 0.041904\n",
      "Train Epoch: 7 [47360/60000 (79%)]\tLoss: 0.008134\n",
      "Train Epoch: 7 [48640/60000 (81%)]\tLoss: 0.042852\n",
      "Train Epoch: 7 [49920/60000 (83%)]\tLoss: 0.007778\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.045547\n",
      "Train Epoch: 7 [52480/60000 (87%)]\tLoss: 0.001632\n",
      "Train Epoch: 7 [53760/60000 (90%)]\tLoss: 0.013530\n",
      "Train Epoch: 7 [55040/60000 (92%)]\tLoss: 0.008496\n",
      "Train Epoch: 7 [56320/60000 (94%)]\tLoss: 0.027433\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.008031\n",
      "Train Epoch: 7 [58880/60000 (98%)]\tLoss: 0.024160\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.007884\n",
      "Train Epoch: 8 [1280/60000 (2%)]\tLoss: 0.009565\n",
      "Train Epoch: 8 [2560/60000 (4%)]\tLoss: 0.023410\n",
      "Train Epoch: 8 [3840/60000 (6%)]\tLoss: 0.002034\n",
      "Train Epoch: 8 [5120/60000 (9%)]\tLoss: 0.005581\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.011799\n",
      "Train Epoch: 8 [7680/60000 (13%)]\tLoss: 0.025991\n",
      "Train Epoch: 8 [8960/60000 (15%)]\tLoss: 0.020784\n",
      "Train Epoch: 8 [10240/60000 (17%)]\tLoss: 0.023203\n",
      "Train Epoch: 8 [11520/60000 (19%)]\tLoss: 0.034036\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.095588\n",
      "Train Epoch: 8 [14080/60000 (23%)]\tLoss: 0.002166\n",
      "Train Epoch: 8 [15360/60000 (26%)]\tLoss: 0.027929\n",
      "Train Epoch: 8 [16640/60000 (28%)]\tLoss: 0.032714\n",
      "Train Epoch: 8 [17920/60000 (30%)]\tLoss: 0.048943\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.006156\n",
      "Train Epoch: 8 [20480/60000 (34%)]\tLoss: 0.002226\n",
      "Train Epoch: 8 [21760/60000 (36%)]\tLoss: 0.021663\n",
      "Train Epoch: 8 [23040/60000 (38%)]\tLoss: 0.035577\n",
      "Train Epoch: 8 [24320/60000 (41%)]\tLoss: 0.007957\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.013116\n",
      "Train Epoch: 8 [26880/60000 (45%)]\tLoss: 0.014144\n",
      "Train Epoch: 8 [28160/60000 (47%)]\tLoss: 0.018248\n",
      "Train Epoch: 8 [29440/60000 (49%)]\tLoss: 0.018576\n",
      "Train Epoch: 8 [30720/60000 (51%)]\tLoss: 0.101679\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.082963\n",
      "Train Epoch: 8 [33280/60000 (55%)]\tLoss: 0.001389\n",
      "Train Epoch: 8 [34560/60000 (58%)]\tLoss: 0.014490\n",
      "Train Epoch: 8 [35840/60000 (60%)]\tLoss: 0.015046\n",
      "Train Epoch: 8 [37120/60000 (62%)]\tLoss: 0.016138\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.019443\n",
      "Train Epoch: 8 [39680/60000 (66%)]\tLoss: 0.019523\n",
      "Train Epoch: 8 [40960/60000 (68%)]\tLoss: 0.003055\n",
      "Train Epoch: 8 [42240/60000 (70%)]\tLoss: 0.005015\n",
      "Train Epoch: 8 [43520/60000 (73%)]\tLoss: 0.028136\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.001488\n",
      "Train Epoch: 8 [46080/60000 (77%)]\tLoss: 0.014959\n",
      "Train Epoch: 8 [47360/60000 (79%)]\tLoss: 0.004606\n",
      "Train Epoch: 8 [48640/60000 (81%)]\tLoss: 0.004865\n",
      "Train Epoch: 8 [49920/60000 (83%)]\tLoss: 0.007566\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.078463\n",
      "Train Epoch: 8 [52480/60000 (87%)]\tLoss: 0.002695\n",
      "Train Epoch: 8 [53760/60000 (90%)]\tLoss: 0.028143\n",
      "Train Epoch: 8 [55040/60000 (92%)]\tLoss: 0.029591\n",
      "Train Epoch: 8 [56320/60000 (94%)]\tLoss: 0.003414\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.013080\n",
      "Train Epoch: 8 [58880/60000 (98%)]\tLoss: 0.007661\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.024322\n",
      "Train Epoch: 9 [1280/60000 (2%)]\tLoss: 0.064990\n",
      "Train Epoch: 9 [2560/60000 (4%)]\tLoss: 0.009470\n",
      "Train Epoch: 9 [3840/60000 (6%)]\tLoss: 0.004180\n",
      "Train Epoch: 9 [5120/60000 (9%)]\tLoss: 0.019059\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.009355\n",
      "Train Epoch: 9 [7680/60000 (13%)]\tLoss: 0.006117\n",
      "Train Epoch: 9 [8960/60000 (15%)]\tLoss: 0.029417\n",
      "Train Epoch: 9 [10240/60000 (17%)]\tLoss: 0.009700\n",
      "Train Epoch: 9 [11520/60000 (19%)]\tLoss: 0.089718\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.010818\n",
      "Train Epoch: 9 [14080/60000 (23%)]\tLoss: 0.041494\n",
      "Train Epoch: 9 [15360/60000 (26%)]\tLoss: 0.052273\n",
      "Train Epoch: 9 [16640/60000 (28%)]\tLoss: 0.038458\n",
      "Train Epoch: 9 [17920/60000 (30%)]\tLoss: 0.003731\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.095211\n",
      "Train Epoch: 9 [20480/60000 (34%)]\tLoss: 0.012052\n",
      "Train Epoch: 9 [21760/60000 (36%)]\tLoss: 0.003716\n",
      "Train Epoch: 9 [23040/60000 (38%)]\tLoss: 0.008091\n",
      "Train Epoch: 9 [24320/60000 (41%)]\tLoss: 0.007652\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.001077\n",
      "Train Epoch: 9 [26880/60000 (45%)]\tLoss: 0.009892\n",
      "Train Epoch: 9 [28160/60000 (47%)]\tLoss: 0.004920\n",
      "Train Epoch: 9 [29440/60000 (49%)]\tLoss: 0.006786\n",
      "Train Epoch: 9 [30720/60000 (51%)]\tLoss: 0.076151\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.007761\n",
      "Train Epoch: 9 [33280/60000 (55%)]\tLoss: 0.011794\n",
      "Train Epoch: 9 [34560/60000 (58%)]\tLoss: 0.010254\n",
      "Train Epoch: 9 [35840/60000 (60%)]\tLoss: 0.096316\n",
      "Train Epoch: 9 [37120/60000 (62%)]\tLoss: 0.017033\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.008918\n",
      "Train Epoch: 9 [39680/60000 (66%)]\tLoss: 0.036842\n",
      "Train Epoch: 9 [40960/60000 (68%)]\tLoss: 0.011339\n",
      "Train Epoch: 9 [42240/60000 (70%)]\tLoss: 0.036403\n",
      "Train Epoch: 9 [43520/60000 (73%)]\tLoss: 0.001345\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.020841\n",
      "Train Epoch: 9 [46080/60000 (77%)]\tLoss: 0.015892\n",
      "Train Epoch: 9 [47360/60000 (79%)]\tLoss: 0.008866\n",
      "Train Epoch: 9 [48640/60000 (81%)]\tLoss: 0.003923\n",
      "Train Epoch: 9 [49920/60000 (83%)]\tLoss: 0.035642\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.002569\n",
      "Train Epoch: 9 [52480/60000 (87%)]\tLoss: 0.001962\n",
      "Train Epoch: 9 [53760/60000 (90%)]\tLoss: 0.015059\n",
      "Train Epoch: 9 [55040/60000 (92%)]\tLoss: 0.013071\n",
      "Train Epoch: 9 [56320/60000 (94%)]\tLoss: 0.001672\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.004887\n",
      "Train Epoch: 9 [58880/60000 (98%)]\tLoss: 0.008486\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.009704\n",
      "Train Epoch: 10 [1280/60000 (2%)]\tLoss: 0.039586\n",
      "Train Epoch: 10 [2560/60000 (4%)]\tLoss: 0.003973\n",
      "Train Epoch: 10 [3840/60000 (6%)]\tLoss: 0.014472\n",
      "Train Epoch: 10 [5120/60000 (9%)]\tLoss: 0.011719\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.020658\n",
      "Train Epoch: 10 [7680/60000 (13%)]\tLoss: 0.050766\n",
      "Train Epoch: 10 [8960/60000 (15%)]\tLoss: 0.019707\n",
      "Train Epoch: 10 [10240/60000 (17%)]\tLoss: 0.001489\n",
      "Train Epoch: 10 [11520/60000 (19%)]\tLoss: 0.071995\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.008559\n",
      "Train Epoch: 10 [14080/60000 (23%)]\tLoss: 0.004260\n",
      "Train Epoch: 10 [15360/60000 (26%)]\tLoss: 0.025984\n",
      "Train Epoch: 10 [16640/60000 (28%)]\tLoss: 0.008573\n",
      "Train Epoch: 10 [17920/60000 (30%)]\tLoss: 0.025492\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.000922\n",
      "Train Epoch: 10 [20480/60000 (34%)]\tLoss: 0.001894\n",
      "Train Epoch: 10 [21760/60000 (36%)]\tLoss: 0.014694\n",
      "Train Epoch: 10 [23040/60000 (38%)]\tLoss: 0.017904\n",
      "Train Epoch: 10 [24320/60000 (41%)]\tLoss: 0.005552\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.004866\n",
      "Train Epoch: 10 [26880/60000 (45%)]\tLoss: 0.006808\n",
      "Train Epoch: 10 [28160/60000 (47%)]\tLoss: 0.005813\n",
      "Train Epoch: 10 [29440/60000 (49%)]\tLoss: 0.002562\n",
      "Train Epoch: 10 [30720/60000 (51%)]\tLoss: 0.011112\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.009913\n",
      "Train Epoch: 10 [33280/60000 (55%)]\tLoss: 0.011628\n",
      "Train Epoch: 10 [34560/60000 (58%)]\tLoss: 0.008711\n",
      "Train Epoch: 10 [35840/60000 (60%)]\tLoss: 0.011254\n",
      "Train Epoch: 10 [37120/60000 (62%)]\tLoss: 0.005157\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.002930\n",
      "Train Epoch: 10 [39680/60000 (66%)]\tLoss: 0.018427\n",
      "Train Epoch: 10 [40960/60000 (68%)]\tLoss: 0.009630\n",
      "Train Epoch: 10 [42240/60000 (70%)]\tLoss: 0.023826\n",
      "Train Epoch: 10 [43520/60000 (73%)]\tLoss: 0.007692\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.020211\n",
      "Train Epoch: 10 [46080/60000 (77%)]\tLoss: 0.018353\n",
      "Train Epoch: 10 [47360/60000 (79%)]\tLoss: 0.028914\n",
      "Train Epoch: 10 [48640/60000 (81%)]\tLoss: 0.004302\n",
      "Train Epoch: 10 [49920/60000 (83%)]\tLoss: 0.098677\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.017227\n",
      "Train Epoch: 10 [52480/60000 (87%)]\tLoss: 0.016240\n",
      "Train Epoch: 10 [53760/60000 (90%)]\tLoss: 0.005648\n",
      "Train Epoch: 10 [55040/60000 (92%)]\tLoss: 0.043742\n",
      "Train Epoch: 10 [56320/60000 (94%)]\tLoss: 0.006779\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.038057\n",
      "Train Epoch: 10 [58880/60000 (98%)]\tLoss: 0.024873\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.002187\n",
      "Train Epoch: 11 [1280/60000 (2%)]\tLoss: 0.008818\n",
      "Train Epoch: 11 [2560/60000 (4%)]\tLoss: 0.048949\n",
      "Train Epoch: 11 [3840/60000 (6%)]\tLoss: 0.012205\n",
      "Train Epoch: 11 [5120/60000 (9%)]\tLoss: 0.015131\n",
      "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 0.010422\n",
      "Train Epoch: 11 [7680/60000 (13%)]\tLoss: 0.009496\n",
      "Train Epoch: 11 [8960/60000 (15%)]\tLoss: 0.006695\n",
      "Train Epoch: 11 [10240/60000 (17%)]\tLoss: 0.010951\n",
      "Train Epoch: 11 [11520/60000 (19%)]\tLoss: 0.061121\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.052139\n",
      "Train Epoch: 11 [14080/60000 (23%)]\tLoss: 0.004597\n",
      "Train Epoch: 11 [15360/60000 (26%)]\tLoss: 0.002508\n",
      "Train Epoch: 11 [16640/60000 (28%)]\tLoss: 0.001197\n",
      "Train Epoch: 11 [17920/60000 (30%)]\tLoss: 0.015841\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 0.000957\n",
      "Train Epoch: 11 [20480/60000 (34%)]\tLoss: 0.004155\n",
      "Train Epoch: 11 [21760/60000 (36%)]\tLoss: 0.008505\n",
      "Train Epoch: 11 [23040/60000 (38%)]\tLoss: 0.010807\n",
      "Train Epoch: 11 [24320/60000 (41%)]\tLoss: 0.004204\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.013393\n",
      "Train Epoch: 11 [26880/60000 (45%)]\tLoss: 0.003463\n",
      "Train Epoch: 11 [28160/60000 (47%)]\tLoss: 0.036164\n",
      "Train Epoch: 11 [29440/60000 (49%)]\tLoss: 0.005981\n",
      "Train Epoch: 11 [30720/60000 (51%)]\tLoss: 0.004376\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.010747\n",
      "Train Epoch: 11 [33280/60000 (55%)]\tLoss: 0.002764\n",
      "Train Epoch: 11 [34560/60000 (58%)]\tLoss: 0.006097\n",
      "Train Epoch: 11 [35840/60000 (60%)]\tLoss: 0.014538\n",
      "Train Epoch: 11 [37120/60000 (62%)]\tLoss: 0.001072\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.003881\n",
      "Train Epoch: 11 [39680/60000 (66%)]\tLoss: 0.012381\n",
      "Train Epoch: 11 [40960/60000 (68%)]\tLoss: 0.024740\n",
      "Train Epoch: 11 [42240/60000 (70%)]\tLoss: 0.012646\n",
      "Train Epoch: 11 [43520/60000 (73%)]\tLoss: 0.011601\n",
      "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 0.004722\n",
      "Train Epoch: 11 [46080/60000 (77%)]\tLoss: 0.003835\n",
      "Train Epoch: 11 [47360/60000 (79%)]\tLoss: 0.042903\n",
      "Train Epoch: 11 [48640/60000 (81%)]\tLoss: 0.006371\n",
      "Train Epoch: 11 [49920/60000 (83%)]\tLoss: 0.007146\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.021240\n",
      "Train Epoch: 11 [52480/60000 (87%)]\tLoss: 0.020779\n",
      "Train Epoch: 11 [53760/60000 (90%)]\tLoss: 0.014220\n",
      "Train Epoch: 11 [55040/60000 (92%)]\tLoss: 0.007113\n",
      "Train Epoch: 11 [56320/60000 (94%)]\tLoss: 0.002312\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 0.001849\n",
      "Train Epoch: 11 [58880/60000 (98%)]\tLoss: 0.011135\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.027548\n",
      "Train Epoch: 12 [1280/60000 (2%)]\tLoss: 0.004442\n",
      "Train Epoch: 12 [2560/60000 (4%)]\tLoss: 0.036504\n",
      "Train Epoch: 12 [3840/60000 (6%)]\tLoss: 0.002930\n",
      "Train Epoch: 12 [5120/60000 (9%)]\tLoss: 0.009673\n",
      "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 0.004496\n",
      "Train Epoch: 12 [7680/60000 (13%)]\tLoss: 0.004989\n",
      "Train Epoch: 12 [8960/60000 (15%)]\tLoss: 0.004513\n",
      "Train Epoch: 12 [10240/60000 (17%)]\tLoss: 0.003682\n",
      "Train Epoch: 12 [11520/60000 (19%)]\tLoss: 0.006277\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.009990\n",
      "Train Epoch: 12 [14080/60000 (23%)]\tLoss: 0.019694\n",
      "Train Epoch: 12 [15360/60000 (26%)]\tLoss: 0.000210\n",
      "Train Epoch: 12 [16640/60000 (28%)]\tLoss: 0.024729\n",
      "Train Epoch: 12 [17920/60000 (30%)]\tLoss: 0.011274\n",
      "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 0.008540\n",
      "Train Epoch: 12 [20480/60000 (34%)]\tLoss: 0.083675\n",
      "Train Epoch: 12 [21760/60000 (36%)]\tLoss: 0.137666\n",
      "Train Epoch: 12 [23040/60000 (38%)]\tLoss: 0.004220\n",
      "Train Epoch: 12 [24320/60000 (41%)]\tLoss: 0.008120\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.005398\n",
      "Train Epoch: 12 [26880/60000 (45%)]\tLoss: 0.002357\n",
      "Train Epoch: 12 [28160/60000 (47%)]\tLoss: 0.010025\n",
      "Train Epoch: 12 [29440/60000 (49%)]\tLoss: 0.003546\n",
      "Train Epoch: 12 [30720/60000 (51%)]\tLoss: 0.001765\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.004769\n",
      "Train Epoch: 12 [33280/60000 (55%)]\tLoss: 0.005380\n",
      "Train Epoch: 12 [34560/60000 (58%)]\tLoss: 0.000633\n",
      "Train Epoch: 12 [35840/60000 (60%)]\tLoss: 0.016618\n",
      "Train Epoch: 12 [37120/60000 (62%)]\tLoss: 0.002259\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.010303\n",
      "Train Epoch: 12 [39680/60000 (66%)]\tLoss: 0.015662\n",
      "Train Epoch: 12 [40960/60000 (68%)]\tLoss: 0.004497\n",
      "Train Epoch: 12 [42240/60000 (70%)]\tLoss: 0.007222\n",
      "Train Epoch: 12 [43520/60000 (73%)]\tLoss: 0.005742\n",
      "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 0.006269\n",
      "Train Epoch: 12 [46080/60000 (77%)]\tLoss: 0.011753\n",
      "Train Epoch: 12 [47360/60000 (79%)]\tLoss: 0.001709\n",
      "Train Epoch: 12 [48640/60000 (81%)]\tLoss: 0.010158\n",
      "Train Epoch: 12 [49920/60000 (83%)]\tLoss: 0.009542\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.092304\n",
      "Train Epoch: 12 [52480/60000 (87%)]\tLoss: 0.005459\n",
      "Train Epoch: 12 [53760/60000 (90%)]\tLoss: 0.016867\n",
      "Train Epoch: 12 [55040/60000 (92%)]\tLoss: 0.014756\n",
      "Train Epoch: 12 [56320/60000 (94%)]\tLoss: 0.007064\n",
      "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 0.000379\n",
      "Train Epoch: 12 [58880/60000 (98%)]\tLoss: 0.011564\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.010535\n",
      "Train Epoch: 13 [1280/60000 (2%)]\tLoss: 0.006771\n",
      "Train Epoch: 13 [2560/60000 (4%)]\tLoss: 0.001124\n",
      "Train Epoch: 13 [3840/60000 (6%)]\tLoss: 0.006731\n",
      "Train Epoch: 13 [5120/60000 (9%)]\tLoss: 0.005013\n",
      "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 0.004520\n",
      "Train Epoch: 13 [7680/60000 (13%)]\tLoss: 0.021125\n",
      "Train Epoch: 13 [8960/60000 (15%)]\tLoss: 0.015326\n",
      "Train Epoch: 13 [10240/60000 (17%)]\tLoss: 0.005004\n",
      "Train Epoch: 13 [11520/60000 (19%)]\tLoss: 0.004229\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.002114\n",
      "Train Epoch: 13 [14080/60000 (23%)]\tLoss: 0.010450\n",
      "Train Epoch: 13 [15360/60000 (26%)]\tLoss: 0.063812\n",
      "Train Epoch: 13 [16640/60000 (28%)]\tLoss: 0.015451\n",
      "Train Epoch: 13 [17920/60000 (30%)]\tLoss: 0.007930\n",
      "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 0.020195\n",
      "Train Epoch: 13 [20480/60000 (34%)]\tLoss: 0.001532\n",
      "Train Epoch: 13 [21760/60000 (36%)]\tLoss: 0.003151\n",
      "Train Epoch: 13 [23040/60000 (38%)]\tLoss: 0.007876\n",
      "Train Epoch: 13 [24320/60000 (41%)]\tLoss: 0.030641\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.012113\n",
      "Train Epoch: 13 [26880/60000 (45%)]\tLoss: 0.050396\n",
      "Train Epoch: 13 [28160/60000 (47%)]\tLoss: 0.000640\n",
      "Train Epoch: 13 [29440/60000 (49%)]\tLoss: 0.009527\n",
      "Train Epoch: 13 [30720/60000 (51%)]\tLoss: 0.015069\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.001798\n",
      "Train Epoch: 13 [33280/60000 (55%)]\tLoss: 0.002718\n",
      "Train Epoch: 13 [34560/60000 (58%)]\tLoss: 0.038281\n",
      "Train Epoch: 13 [35840/60000 (60%)]\tLoss: 0.018673\n",
      "Train Epoch: 13 [37120/60000 (62%)]\tLoss: 0.019081\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.003735\n",
      "Train Epoch: 13 [39680/60000 (66%)]\tLoss: 0.005961\n",
      "Train Epoch: 13 [40960/60000 (68%)]\tLoss: 0.003440\n",
      "Train Epoch: 13 [42240/60000 (70%)]\tLoss: 0.016305\n",
      "Train Epoch: 13 [43520/60000 (73%)]\tLoss: 0.013097\n",
      "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 0.010914\n",
      "Train Epoch: 13 [46080/60000 (77%)]\tLoss: 0.028618\n",
      "Train Epoch: 13 [47360/60000 (79%)]\tLoss: 0.001923\n",
      "Train Epoch: 13 [48640/60000 (81%)]\tLoss: 0.005211\n",
      "Train Epoch: 13 [49920/60000 (83%)]\tLoss: 0.045091\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.014145\n",
      "Train Epoch: 13 [52480/60000 (87%)]\tLoss: 0.001104\n",
      "Train Epoch: 13 [53760/60000 (90%)]\tLoss: 0.006286\n",
      "Train Epoch: 13 [55040/60000 (92%)]\tLoss: 0.006825\n",
      "Train Epoch: 13 [56320/60000 (94%)]\tLoss: 0.001789\n",
      "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 0.002610\n",
      "Train Epoch: 13 [58880/60000 (98%)]\tLoss: 0.001372\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.003132\n",
      "Train Epoch: 14 [1280/60000 (2%)]\tLoss: 0.010694\n",
      "Train Epoch: 14 [2560/60000 (4%)]\tLoss: 0.007265\n",
      "Train Epoch: 14 [3840/60000 (6%)]\tLoss: 0.008024\n",
      "Train Epoch: 14 [5120/60000 (9%)]\tLoss: 0.015161\n",
      "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 0.000649\n",
      "Train Epoch: 14 [7680/60000 (13%)]\tLoss: 0.008142\n",
      "Train Epoch: 14 [8960/60000 (15%)]\tLoss: 0.008836\n",
      "Train Epoch: 14 [10240/60000 (17%)]\tLoss: 0.005668\n",
      "Train Epoch: 14 [11520/60000 (19%)]\tLoss: 0.004469\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 0.022803\n",
      "Train Epoch: 14 [14080/60000 (23%)]\tLoss: 0.020188\n",
      "Train Epoch: 14 [15360/60000 (26%)]\tLoss: 0.004270\n",
      "Train Epoch: 14 [16640/60000 (28%)]\tLoss: 0.005466\n",
      "Train Epoch: 14 [17920/60000 (30%)]\tLoss: 0.005583\n",
      "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 0.007442\n",
      "Train Epoch: 14 [20480/60000 (34%)]\tLoss: 0.002700\n",
      "Train Epoch: 14 [21760/60000 (36%)]\tLoss: 0.001665\n",
      "Train Epoch: 14 [23040/60000 (38%)]\tLoss: 0.000365\n",
      "Train Epoch: 14 [24320/60000 (41%)]\tLoss: 0.003728\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.003859\n",
      "Train Epoch: 14 [26880/60000 (45%)]\tLoss: 0.007200\n",
      "Train Epoch: 14 [28160/60000 (47%)]\tLoss: 0.003532\n",
      "Train Epoch: 14 [29440/60000 (49%)]\tLoss: 0.002893\n",
      "Train Epoch: 14 [30720/60000 (51%)]\tLoss: 0.007958\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 0.000639\n",
      "Train Epoch: 14 [33280/60000 (55%)]\tLoss: 0.007050\n",
      "Train Epoch: 14 [34560/60000 (58%)]\tLoss: 0.017378\n",
      "Train Epoch: 14 [35840/60000 (60%)]\tLoss: 0.013172\n",
      "Train Epoch: 14 [37120/60000 (62%)]\tLoss: 0.005704\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.001638\n",
      "Train Epoch: 14 [39680/60000 (66%)]\tLoss: 0.003855\n",
      "Train Epoch: 14 [40960/60000 (68%)]\tLoss: 0.001606\n",
      "Train Epoch: 14 [42240/60000 (70%)]\tLoss: 0.002726\n",
      "Train Epoch: 14 [43520/60000 (73%)]\tLoss: 0.017578\n",
      "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 0.019594\n",
      "Train Epoch: 14 [46080/60000 (77%)]\tLoss: 0.016861\n",
      "Train Epoch: 14 [47360/60000 (79%)]\tLoss: 0.003259\n",
      "Train Epoch: 14 [48640/60000 (81%)]\tLoss: 0.021514\n",
      "Train Epoch: 14 [49920/60000 (83%)]\tLoss: 0.003814\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.001115\n",
      "Train Epoch: 14 [52480/60000 (87%)]\tLoss: 0.011126\n",
      "Train Epoch: 14 [53760/60000 (90%)]\tLoss: 0.003205\n",
      "Train Epoch: 14 [55040/60000 (92%)]\tLoss: 0.053553\n",
      "Train Epoch: 14 [56320/60000 (94%)]\tLoss: 0.002003\n",
      "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 0.021388\n",
      "Train Epoch: 14 [58880/60000 (98%)]\tLoss: 0.007290\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.013099\n",
      "Train Epoch: 15 [1280/60000 (2%)]\tLoss: 0.001896\n",
      "Train Epoch: 15 [2560/60000 (4%)]\tLoss: 0.008840\n",
      "Train Epoch: 15 [3840/60000 (6%)]\tLoss: 0.007865\n",
      "Train Epoch: 15 [5120/60000 (9%)]\tLoss: 0.004444\n",
      "Train Epoch: 15 [6400/60000 (11%)]\tLoss: 0.003637\n",
      "Train Epoch: 15 [7680/60000 (13%)]\tLoss: 0.009096\n",
      "Train Epoch: 15 [8960/60000 (15%)]\tLoss: 0.004294\n",
      "Train Epoch: 15 [10240/60000 (17%)]\tLoss: 0.005876\n",
      "Train Epoch: 15 [11520/60000 (19%)]\tLoss: 0.011062\n",
      "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 0.007252\n",
      "Train Epoch: 15 [14080/60000 (23%)]\tLoss: 0.016209\n",
      "Train Epoch: 15 [15360/60000 (26%)]\tLoss: 0.006378\n",
      "Train Epoch: 15 [16640/60000 (28%)]\tLoss: 0.002169\n",
      "Train Epoch: 15 [17920/60000 (30%)]\tLoss: 0.003098\n",
      "Train Epoch: 15 [19200/60000 (32%)]\tLoss: 0.001140\n",
      "Train Epoch: 15 [20480/60000 (34%)]\tLoss: 0.006090\n",
      "Train Epoch: 15 [21760/60000 (36%)]\tLoss: 0.009399\n",
      "Train Epoch: 15 [23040/60000 (38%)]\tLoss: 0.019175\n",
      "Train Epoch: 15 [24320/60000 (41%)]\tLoss: 0.001932\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.022759\n",
      "Train Epoch: 15 [26880/60000 (45%)]\tLoss: 0.002435\n",
      "Train Epoch: 15 [28160/60000 (47%)]\tLoss: 0.013944\n",
      "Train Epoch: 15 [29440/60000 (49%)]\tLoss: 0.016037\n",
      "Train Epoch: 15 [30720/60000 (51%)]\tLoss: 0.005310\n",
      "Train Epoch: 15 [32000/60000 (53%)]\tLoss: 0.001047\n",
      "Train Epoch: 15 [33280/60000 (55%)]\tLoss: 0.000548\n",
      "Train Epoch: 15 [34560/60000 (58%)]\tLoss: 0.028435\n",
      "Train Epoch: 15 [35840/60000 (60%)]\tLoss: 0.004571\n",
      "Train Epoch: 15 [37120/60000 (62%)]\tLoss: 0.003017\n",
      "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 0.012021\n",
      "Train Epoch: 15 [39680/60000 (66%)]\tLoss: 0.030711\n",
      "Train Epoch: 15 [40960/60000 (68%)]\tLoss: 0.004723\n",
      "Train Epoch: 15 [42240/60000 (70%)]\tLoss: 0.006904\n",
      "Train Epoch: 15 [43520/60000 (73%)]\tLoss: 0.010769\n",
      "Train Epoch: 15 [44800/60000 (75%)]\tLoss: 0.002347\n",
      "Train Epoch: 15 [46080/60000 (77%)]\tLoss: 0.030962\n",
      "Train Epoch: 15 [47360/60000 (79%)]\tLoss: 0.011926\n",
      "Train Epoch: 15 [48640/60000 (81%)]\tLoss: 0.007390\n",
      "Train Epoch: 15 [49920/60000 (83%)]\tLoss: 0.002514\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.003048\n",
      "Train Epoch: 15 [52480/60000 (87%)]\tLoss: 0.001530\n",
      "Train Epoch: 15 [53760/60000 (90%)]\tLoss: 0.010017\n",
      "Train Epoch: 15 [55040/60000 (92%)]\tLoss: 0.162510\n",
      "Train Epoch: 15 [56320/60000 (94%)]\tLoss: 0.001398\n",
      "Train Epoch: 15 [57600/60000 (96%)]\tLoss: 0.068437\n",
      "Train Epoch: 15 [58880/60000 (98%)]\tLoss: 0.022294\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.008500\n",
      "Train Epoch: 16 [1280/60000 (2%)]\tLoss: 0.009606\n",
      "Train Epoch: 16 [2560/60000 (4%)]\tLoss: 0.011733\n",
      "Train Epoch: 16 [3840/60000 (6%)]\tLoss: 0.004520\n",
      "Train Epoch: 16 [5120/60000 (9%)]\tLoss: 0.012074\n",
      "Train Epoch: 16 [6400/60000 (11%)]\tLoss: 0.037273\n",
      "Train Epoch: 16 [7680/60000 (13%)]\tLoss: 0.001010\n",
      "Train Epoch: 16 [8960/60000 (15%)]\tLoss: 0.000178\n",
      "Train Epoch: 16 [10240/60000 (17%)]\tLoss: 0.006094\n",
      "Train Epoch: 16 [11520/60000 (19%)]\tLoss: 0.001275\n",
      "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 0.013027\n",
      "Train Epoch: 16 [14080/60000 (23%)]\tLoss: 0.009817\n",
      "Train Epoch: 16 [15360/60000 (26%)]\tLoss: 0.004779\n",
      "Train Epoch: 16 [16640/60000 (28%)]\tLoss: 0.001590\n",
      "Train Epoch: 16 [17920/60000 (30%)]\tLoss: 0.003985\n",
      "Train Epoch: 16 [19200/60000 (32%)]\tLoss: 0.002303\n",
      "Train Epoch: 16 [20480/60000 (34%)]\tLoss: 0.001125\n",
      "Train Epoch: 16 [21760/60000 (36%)]\tLoss: 0.004145\n",
      "Train Epoch: 16 [23040/60000 (38%)]\tLoss: 0.003968\n",
      "Train Epoch: 16 [24320/60000 (41%)]\tLoss: 0.002245\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.003261\n",
      "Train Epoch: 16 [26880/60000 (45%)]\tLoss: 0.005297\n",
      "Train Epoch: 16 [28160/60000 (47%)]\tLoss: 0.000256\n",
      "Train Epoch: 16 [29440/60000 (49%)]\tLoss: 0.004666\n",
      "Train Epoch: 16 [30720/60000 (51%)]\tLoss: 0.005284\n",
      "Train Epoch: 16 [32000/60000 (53%)]\tLoss: 0.000526\n",
      "Train Epoch: 16 [33280/60000 (55%)]\tLoss: 0.004390\n",
      "Train Epoch: 16 [34560/60000 (58%)]\tLoss: 0.006027\n",
      "Train Epoch: 16 [35840/60000 (60%)]\tLoss: 0.021538\n",
      "Train Epoch: 16 [37120/60000 (62%)]\tLoss: 0.012662\n",
      "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 0.003628\n",
      "Train Epoch: 16 [39680/60000 (66%)]\tLoss: 0.007913\n",
      "Train Epoch: 16 [40960/60000 (68%)]\tLoss: 0.020807\n",
      "Train Epoch: 16 [42240/60000 (70%)]\tLoss: 0.002336\n",
      "Train Epoch: 16 [43520/60000 (73%)]\tLoss: 0.006067\n",
      "Train Epoch: 16 [44800/60000 (75%)]\tLoss: 0.007266\n",
      "Train Epoch: 16 [46080/60000 (77%)]\tLoss: 0.008239\n",
      "Train Epoch: 16 [47360/60000 (79%)]\tLoss: 0.008549\n",
      "Train Epoch: 16 [48640/60000 (81%)]\tLoss: 0.013400\n",
      "Train Epoch: 16 [49920/60000 (83%)]\tLoss: 0.003204\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.001621\n",
      "Train Epoch: 16 [52480/60000 (87%)]\tLoss: 0.003867\n",
      "Train Epoch: 16 [53760/60000 (90%)]\tLoss: 0.001543\n",
      "Train Epoch: 16 [55040/60000 (92%)]\tLoss: 0.038471\n",
      "Train Epoch: 16 [56320/60000 (94%)]\tLoss: 0.006553\n",
      "Train Epoch: 16 [57600/60000 (96%)]\tLoss: 0.013421\n",
      "Train Epoch: 16 [58880/60000 (98%)]\tLoss: 0.001962\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.006079\n",
      "Train Epoch: 17 [1280/60000 (2%)]\tLoss: 0.011694\n",
      "Train Epoch: 17 [2560/60000 (4%)]\tLoss: 0.001226\n",
      "Train Epoch: 17 [3840/60000 (6%)]\tLoss: 0.016287\n",
      "Train Epoch: 17 [5120/60000 (9%)]\tLoss: 0.009116\n",
      "Train Epoch: 17 [6400/60000 (11%)]\tLoss: 0.004096\n",
      "Train Epoch: 17 [7680/60000 (13%)]\tLoss: 0.003886\n",
      "Train Epoch: 17 [8960/60000 (15%)]\tLoss: 0.002079\n",
      "Train Epoch: 17 [10240/60000 (17%)]\tLoss: 0.006643\n",
      "Train Epoch: 17 [11520/60000 (19%)]\tLoss: 0.001761\n",
      "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 0.006035\n",
      "Train Epoch: 17 [14080/60000 (23%)]\tLoss: 0.001190\n",
      "Train Epoch: 17 [15360/60000 (26%)]\tLoss: 0.000727\n",
      "Train Epoch: 17 [16640/60000 (28%)]\tLoss: 0.017691\n",
      "Train Epoch: 17 [17920/60000 (30%)]\tLoss: 0.006106\n",
      "Train Epoch: 17 [19200/60000 (32%)]\tLoss: 0.001589\n",
      "Train Epoch: 17 [20480/60000 (34%)]\tLoss: 0.012640\n",
      "Train Epoch: 17 [21760/60000 (36%)]\tLoss: 0.011896\n",
      "Train Epoch: 17 [23040/60000 (38%)]\tLoss: 0.003270\n",
      "Train Epoch: 17 [24320/60000 (41%)]\tLoss: 0.005572\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.002398\n",
      "Train Epoch: 17 [26880/60000 (45%)]\tLoss: 0.003727\n",
      "Train Epoch: 17 [28160/60000 (47%)]\tLoss: 0.002607\n",
      "Train Epoch: 17 [29440/60000 (49%)]\tLoss: 0.012621\n",
      "Train Epoch: 17 [30720/60000 (51%)]\tLoss: 0.004748\n",
      "Train Epoch: 17 [32000/60000 (53%)]\tLoss: 0.000740\n",
      "Train Epoch: 17 [33280/60000 (55%)]\tLoss: 0.016513\n",
      "Train Epoch: 17 [34560/60000 (58%)]\tLoss: 0.005283\n",
      "Train Epoch: 17 [35840/60000 (60%)]\tLoss: 0.006799\n",
      "Train Epoch: 17 [37120/60000 (62%)]\tLoss: 0.012841\n",
      "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 0.007005\n",
      "Train Epoch: 17 [39680/60000 (66%)]\tLoss: 0.019527\n",
      "Train Epoch: 17 [40960/60000 (68%)]\tLoss: 0.030015\n",
      "Train Epoch: 17 [42240/60000 (70%)]\tLoss: 0.005315\n",
      "Train Epoch: 17 [43520/60000 (73%)]\tLoss: 0.010801\n",
      "Train Epoch: 17 [44800/60000 (75%)]\tLoss: 0.004555\n",
      "Train Epoch: 17 [46080/60000 (77%)]\tLoss: 0.002646\n",
      "Train Epoch: 17 [47360/60000 (79%)]\tLoss: 0.009626\n",
      "Train Epoch: 17 [48640/60000 (81%)]\tLoss: 0.007064\n",
      "Train Epoch: 17 [49920/60000 (83%)]\tLoss: 0.004771\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.007479\n",
      "Train Epoch: 17 [52480/60000 (87%)]\tLoss: 0.006556\n",
      "Train Epoch: 17 [53760/60000 (90%)]\tLoss: 0.005949\n",
      "Train Epoch: 17 [55040/60000 (92%)]\tLoss: 0.001288\n",
      "Train Epoch: 17 [56320/60000 (94%)]\tLoss: 0.003738\n",
      "Train Epoch: 17 [57600/60000 (96%)]\tLoss: 0.010378\n",
      "Train Epoch: 17 [58880/60000 (98%)]\tLoss: 0.004108\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.004319\n",
      "Train Epoch: 18 [1280/60000 (2%)]\tLoss: 0.002216\n",
      "Train Epoch: 18 [2560/60000 (4%)]\tLoss: 0.002571\n",
      "Train Epoch: 18 [3840/60000 (6%)]\tLoss: 0.001086\n",
      "Train Epoch: 18 [5120/60000 (9%)]\tLoss: 0.000102\n",
      "Train Epoch: 18 [6400/60000 (11%)]\tLoss: 0.006884\n",
      "Train Epoch: 18 [7680/60000 (13%)]\tLoss: 0.001492\n",
      "Train Epoch: 18 [8960/60000 (15%)]\tLoss: 0.016083\n",
      "Train Epoch: 18 [10240/60000 (17%)]\tLoss: 0.000765\n",
      "Train Epoch: 18 [11520/60000 (19%)]\tLoss: 0.003382\n",
      "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 0.001934\n",
      "Train Epoch: 18 [14080/60000 (23%)]\tLoss: 0.003908\n",
      "Train Epoch: 18 [15360/60000 (26%)]\tLoss: 0.035451\n",
      "Train Epoch: 18 [16640/60000 (28%)]\tLoss: 0.014858\n",
      "Train Epoch: 18 [17920/60000 (30%)]\tLoss: 0.002158\n",
      "Train Epoch: 18 [19200/60000 (32%)]\tLoss: 0.003778\n",
      "Train Epoch: 18 [20480/60000 (34%)]\tLoss: 0.001513\n",
      "Train Epoch: 18 [21760/60000 (36%)]\tLoss: 0.003122\n",
      "Train Epoch: 18 [23040/60000 (38%)]\tLoss: 0.000384\n",
      "Train Epoch: 18 [24320/60000 (41%)]\tLoss: 0.033592\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.014376\n",
      "Train Epoch: 18 [26880/60000 (45%)]\tLoss: 0.002614\n",
      "Train Epoch: 18 [28160/60000 (47%)]\tLoss: 0.003828\n",
      "Train Epoch: 18 [29440/60000 (49%)]\tLoss: 0.001658\n",
      "Train Epoch: 18 [30720/60000 (51%)]\tLoss: 0.000955\n",
      "Train Epoch: 18 [32000/60000 (53%)]\tLoss: 0.008192\n",
      "Train Epoch: 18 [33280/60000 (55%)]\tLoss: 0.006509\n",
      "Train Epoch: 18 [34560/60000 (58%)]\tLoss: 0.010545\n",
      "Train Epoch: 18 [35840/60000 (60%)]\tLoss: 0.000488\n",
      "Train Epoch: 18 [37120/60000 (62%)]\tLoss: 0.004800\n",
      "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 0.002175\n",
      "Train Epoch: 18 [39680/60000 (66%)]\tLoss: 0.002352\n",
      "Train Epoch: 18 [40960/60000 (68%)]\tLoss: 0.000759\n",
      "Train Epoch: 18 [42240/60000 (70%)]\tLoss: 0.012875\n",
      "Train Epoch: 18 [43520/60000 (73%)]\tLoss: 0.011050\n",
      "Train Epoch: 18 [44800/60000 (75%)]\tLoss: 0.005852\n",
      "Train Epoch: 18 [46080/60000 (77%)]\tLoss: 0.003380\n",
      "Train Epoch: 18 [47360/60000 (79%)]\tLoss: 0.018946\n",
      "Train Epoch: 18 [48640/60000 (81%)]\tLoss: 0.001520\n",
      "Train Epoch: 18 [49920/60000 (83%)]\tLoss: 0.001579\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.001453\n",
      "Train Epoch: 18 [52480/60000 (87%)]\tLoss: 0.001106\n",
      "Train Epoch: 18 [53760/60000 (90%)]\tLoss: 0.046623\n",
      "Train Epoch: 18 [55040/60000 (92%)]\tLoss: 0.001037\n",
      "Train Epoch: 18 [56320/60000 (94%)]\tLoss: 0.015531\n",
      "Train Epoch: 18 [57600/60000 (96%)]\tLoss: 0.001757\n",
      "Train Epoch: 18 [58880/60000 (98%)]\tLoss: 0.013505\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.016671\n",
      "Train Epoch: 19 [1280/60000 (2%)]\tLoss: 0.003361\n",
      "Train Epoch: 19 [2560/60000 (4%)]\tLoss: 0.001927\n",
      "Train Epoch: 19 [3840/60000 (6%)]\tLoss: 0.003057\n",
      "Train Epoch: 19 [5120/60000 (9%)]\tLoss: 0.004457\n",
      "Train Epoch: 19 [6400/60000 (11%)]\tLoss: 0.003330\n",
      "Train Epoch: 19 [7680/60000 (13%)]\tLoss: 0.005269\n",
      "Train Epoch: 19 [8960/60000 (15%)]\tLoss: 0.008385\n",
      "Train Epoch: 19 [10240/60000 (17%)]\tLoss: 0.005142\n",
      "Train Epoch: 19 [11520/60000 (19%)]\tLoss: 0.005952\n",
      "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 0.003129\n",
      "Train Epoch: 19 [14080/60000 (23%)]\tLoss: 0.048304\n",
      "Train Epoch: 19 [15360/60000 (26%)]\tLoss: 0.002572\n",
      "Train Epoch: 19 [16640/60000 (28%)]\tLoss: 0.017782\n",
      "Train Epoch: 19 [17920/60000 (30%)]\tLoss: 0.019636\n",
      "Train Epoch: 19 [19200/60000 (32%)]\tLoss: 0.012874\n",
      "Train Epoch: 19 [20480/60000 (34%)]\tLoss: 0.000907\n",
      "Train Epoch: 19 [21760/60000 (36%)]\tLoss: 0.004593\n",
      "Train Epoch: 19 [23040/60000 (38%)]\tLoss: 0.002658\n",
      "Train Epoch: 19 [24320/60000 (41%)]\tLoss: 0.011536\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 0.006239\n",
      "Train Epoch: 19 [26880/60000 (45%)]\tLoss: 0.004993\n",
      "Train Epoch: 19 [28160/60000 (47%)]\tLoss: 0.024157\n",
      "Train Epoch: 19 [29440/60000 (49%)]\tLoss: 0.000337\n",
      "Train Epoch: 19 [30720/60000 (51%)]\tLoss: 0.024807\n",
      "Train Epoch: 19 [32000/60000 (53%)]\tLoss: 0.014894\n",
      "Train Epoch: 19 [33280/60000 (55%)]\tLoss: 0.000180\n",
      "Train Epoch: 19 [34560/60000 (58%)]\tLoss: 0.041361\n",
      "Train Epoch: 19 [35840/60000 (60%)]\tLoss: 0.006628\n",
      "Train Epoch: 19 [37120/60000 (62%)]\tLoss: 0.004853\n",
      "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 0.003573\n",
      "Train Epoch: 19 [39680/60000 (66%)]\tLoss: 0.002423\n",
      "Train Epoch: 19 [40960/60000 (68%)]\tLoss: 0.005123\n",
      "Train Epoch: 19 [42240/60000 (70%)]\tLoss: 0.004501\n",
      "Train Epoch: 19 [43520/60000 (73%)]\tLoss: 0.001029\n",
      "Train Epoch: 19 [44800/60000 (75%)]\tLoss: 0.005552\n",
      "Train Epoch: 19 [46080/60000 (77%)]\tLoss: 0.011148\n",
      "Train Epoch: 19 [47360/60000 (79%)]\tLoss: 0.005243\n",
      "Train Epoch: 19 [48640/60000 (81%)]\tLoss: 0.002045\n",
      "Train Epoch: 19 [49920/60000 (83%)]\tLoss: 0.011180\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.011388\n",
      "Train Epoch: 19 [52480/60000 (87%)]\tLoss: 0.003216\n",
      "Train Epoch: 19 [53760/60000 (90%)]\tLoss: 0.013678\n",
      "Train Epoch: 19 [55040/60000 (92%)]\tLoss: 0.017726\n",
      "Train Epoch: 19 [56320/60000 (94%)]\tLoss: 0.010451\n",
      "Train Epoch: 19 [57600/60000 (96%)]\tLoss: 0.000856\n",
      "Train Epoch: 19 [58880/60000 (98%)]\tLoss: 0.009407\n"
     ]
    }
   ],
   "source": [
    "# 5) Train\n",
    "epochs = 20\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = get_train_loader(batch_size)\n",
    "log_interval=40\n",
    "\n",
    "# run the main training loop\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        # resize data from (batch_size, 1, 28, 28) to (batch_size, 28*28)\n",
    "        data = data.view(-1, 28*28)\n",
    "        optimizer.zero_grad()\n",
    "        net_out = net(data)\n",
    "        loss = criterion(net_out, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                           100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaime\\Anaconda3\\envs\\cvision\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  import sys\n",
      "C:\\Users\\jaime\\Anaconda3\\envs\\cvision\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Users\\jaime\\Anaconda3\\envs\\cvision\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0178, Accuracy: 9805/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6) Testing\n",
    "\n",
    "# run a test loop\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "for data, target in test_loader:\n",
    "    data, target = Variable(data, volatile=True), Variable(target)\n",
    "    data = data.view(-1, 28 * 28)\n",
    "    net_out = net(data)\n",
    "    # sum up batch loss\n",
    "    test_loss += criterion(net_out, target).data[0]\n",
    "    pred = net_out.data.max(1)[1]  # get the index of the max log-probability\n",
    "    correct += pred.eq(target.data).sum()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
