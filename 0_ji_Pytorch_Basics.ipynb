{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks in Pytorch\n",
    "\n",
    "- Ref: Pytorch tutorial: https://pytorch.org/tutorials/\n",
    "- Tensor replaces numpy.ndarray to allow GPU computation!\n",
    "\n",
    "- Ref for this tutorial: https://hsaghir.github.io/data_science/pytorch_starter/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Import Stuff\n",
    "import torch\n",
    "import torch.nn as nn # Neural net library\n",
    "import torch.optim as optim # Optimization library\n",
    "import torch.nn.functional as F # Non-linear functions\n",
    "import torch.autograd as autograd #build a computational graph\n",
    "\n",
    "from torch.autograd import Variable # Computation Graph module to get Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original shape: torch.Size([1, 2, 1, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Two useful functions: \n",
    "# - squeeze: remove dimension of 1\n",
    "# - unsqueeze: insert dimentions of 1\n",
    "\n",
    "# squeeze\n",
    "y = torch.zeros(1,2,1,2)\n",
    "print('original shape:',y.shape)\n",
    "torch.squeeze(y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 1, 4, 5])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unsqueeze\n",
    "z = torch.zeros(2,3,4,5)\n",
    "torch.unsqueeze(z,2).shape # insert dimention 1 at position 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU or CPU\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Replace numpy ndarrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of the tensor: torch.Size([2, 2, 3])\n",
      "adding up the two matrices of the 3d tensor: \n",
      "  8  10  12\n",
      " 15  17  19\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 matrices of size 2x3 into a 3d tensor 2x2x3\n",
    "d = [ [[1., 2.,3.], [4.,5.,6.]], [[7.,8.,9.], [11.,12.,13.]] ]\n",
    "d = torch.Tensor(d) # array from python list\n",
    "print(\"shape of the tensor:\", d.size())\n",
    "\n",
    "# the first index is the depth\n",
    "z = d[0] + d[1]\n",
    "print(\"adding up the two matrices of the 3d tensor:\",z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  1   2   3   4   5   6\n",
      "  7   8   9  11  12  13\n",
      "[torch.FloatTensor of size 2x6]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# a heavily used operation is reshaping of tensors using .view()\n",
    "print(d.view(2,-1)) #-1 makes torch infer the second dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Computational graphs:  torch.autograd\n",
    "\n",
    "- Tensor --> node in the graph\n",
    "- Operations on tensors --> edges in the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the node's data is the tensor: torch.Size([2, 2, 3])\n",
      "the node's gradient is empty at creation: None\n"
     ]
    }
   ],
   "source": [
    "# d is a tensor not a node, to create a node based on it:\n",
    "x = autograd.Variable(d, requires_grad=True)\n",
    "print(\"the node's data is the tensor:\", x.data.size())\n",
    "print(\"the node's gradient is empty at creation:\", x.grad) # the grad is empty right now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SumBackward0 object at 0x0000016B755D6518>\n"
     ]
    }
   ],
   "source": [
    "# do operation on the node to make a computational graph\n",
    "y = x + 1\n",
    "z = x + y\n",
    "s = z.sum()\n",
    "print(s.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the variable now has gradients: Variable containing:\n",
      "(0 ,.,.) = \n",
      "  2  2  2\n",
      "  2  2  2\n",
      "\n",
      "(1 ,.,.) = \n",
      "  2  2  2\n",
      "  2  2  2\n",
      "[torch.FloatTensor of size 2x2x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculate gradients\n",
    "s.backward()\n",
    "print(\"the variable now has gradients:\",x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) torch.nn contains various NN layers\n",
    "\n",
    "- (linear mappings of rows of a tensor) + (nonlinearities)\n",
    "\n",
    "- It helps build a neural net computational graph without the hassle of manipulating tensors and parameters manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using randomly initialized params: <bound method Module.parameters of Linear(in_features=5, out_features=3, bias=True)>\n"
     ]
    }
   ],
   "source": [
    "# linear transformation of a 2x5 matrix into a 2x3 matrix\n",
    "linear_map = nn.Linear(5,3) # input size:5, output size:3\n",
    "print(\"using randomly initialized params:\", linear_map.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output of softmax as a probability distribution: \n",
      " 0.3886  0.3579  0.2535  0.3448  0.3415  0.3137\n",
      "[torch.FloatTensor of size 1x6]\n",
      "\n",
      "Loss: Variable containing:\n",
      " 2.5171\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaime\\Anaconda3\\envs\\cvision\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "# data has 2 examples with 5 features and 3 target\n",
    "data = torch.randn(2,5) # training\n",
    "y = autograd.Variable(torch.randn(2,3)) # target\n",
    "# make a node\n",
    "x = autograd.Variable(data, requires_grad=True)\n",
    "\n",
    "# apply transformation to a node creates a computational graph\n",
    "a = linear_map(x)\n",
    "z = F.relu(a)\n",
    "o = F.softmax(z)\n",
    "print(\"output of softmax as a probability distribution:\", o.data.view(1,-1))\n",
    "\n",
    "# loss function\n",
    "loss_func = nn.MSELoss() #instantiate loss function\n",
    "L = loss_func(z,y) # calculateMSE loss between output and target\n",
    "print(\"Loss:\", L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When defining a custom layer, 2 functions need to be implemented:\n",
    "\n",
    "- \"\\__init\\__\" function has to always be inherited first, then define parameters of the layer here as the class variables i.e. self.x\n",
    "\n",
    "- forward funtion is where we pass an input through the layer, perform operations on inputs using parameters and return the output. The input needs to be an autograd.Variable() so that pytorch can build the computational graph of the layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Log_reg_classifier(nn.Module):\n",
    "    def __init__(self, in_size,out_size):\n",
    "        super(Log_reg_classifier,self).__init__() #always call parent's init \n",
    "        self.linear = nn.Linear(in_size, out_size) #layer parameters\n",
    "        \n",
    "    def forward(self,vect):\n",
    "        return F.log_softmax(self.linear(vect)) # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Optimization\n",
    "\n",
    "- torch.optim can do optimization\n",
    "\n",
    "- we build a nn computational graph using torch.nn, compute gradients using torch.autograd, and then feed them into torch.optim to update network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 2.5171\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jaime\\Anaconda3\\envs\\cvision\\lib\\site-packages\\torch\\autograd\\__init__.py:93: UserWarning: retain_variables option is deprecated and will be removed in 0.3. Use retain_graph instead.\n",
      "  warnings.warn(\"retain_variables option is deprecated and will be removed in 0.3. \"\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(linear_map.parameters(), lr = 1e-2) # instantiate optimizer with model params + learning rate\n",
    "\n",
    "# epoch loop: we run following until convergence\n",
    "optimizer.zero_grad() # make gradients zero\n",
    "L.backward(retain_variables = True)\n",
    "optimizer.step()\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Simple Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, loss 6516.1152 \n",
      "Episode 50, loss 2819.0657 \n",
      "Episode 100, loss 982.0585 \n",
      "Episode 150, loss 270.0243 \n",
      "Episode 200, loss 57.2376 \n",
      "Episode 250, loss 9.3092 \n",
      "Episode 300, loss 1.2464 \n",
      "Episode 350, loss 0.2372 \n",
      "Episode 400, loss 0.1430 \n",
      "Episode 450, loss 0.1353 \n",
      "Episode 500, loss 0.1334 \n",
      "Episode 550, loss 0.1317 \n",
      "Episode 600, loss 0.1300 \n",
      "Episode 650, loss 0.1282 \n",
      "Episode 700, loss 0.1263 \n",
      "Episode 750, loss 0.1244 \n",
      "Episode 800, loss 0.1223 \n",
      "Episode 850, loss 0.1203 \n",
      "Episode 900, loss 0.1182 \n",
      "Episode 950, loss 0.1160 \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# if gpu is to be used\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "W = 2\n",
    "b = 0.3\n",
    "\n",
    "x = torch.arange(100).to(device).unsqueeze(1)\n",
    "\n",
    "y = W * x + b\n",
    "\n",
    "###### PARAMS ######\n",
    "learning_rate = 0.01\n",
    "num_episodes = 1000\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear1 = nn.Linear(1,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.linear1(x)\n",
    "        return output\n",
    "    \n",
    "mynn = NeuralNetwork().to(device)\n",
    "    \n",
    "loss_func = nn.MSELoss()\n",
    "#loss_func = nn.SmoothL1Loss()\n",
    "\n",
    "optimizer = optim.Adam(params=mynn.parameters(), lr=learning_rate)\n",
    "#optimizer = optim.RMSprop(params=mynn.parameters(), lr=learning_rate)\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    \n",
    "    predicted_value = mynn(x)\n",
    "    \n",
    "    loss = loss_func(predicted_value, y)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i_episode % 50 == 0:\n",
    "        print(\"Episode %i, loss %.4f \" % (i_episode, loss.item()))\n",
    "    \n",
    "    \n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(x.cpu().numpy(), y.cpu().numpy(), alpha=0.6, color='green')\n",
    "plt.plot(x.cpu().numpy(), predicted_value.detach().cpu().numpy(), alpha=0.6, color='red')\n",
    "\n",
    "if use_cuda:\n",
    "    plt.savefig(\"graph.png\")\n",
    "else:\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.1.post2'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example (not working...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-cc80fe215f0b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# send data through model in minibatches for 10 epochs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# pytorch accumulates gradients, making them zero for each minibatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Log_reg_classifier(10,2)\n",
    "\n",
    "# define loss function\n",
    "loss_func = nn.MSELoss() \n",
    "\n",
    "# define optimizer\n",
    "optimizer = optim.SGD(model.parameters(),lr=1e-1)\n",
    "\n",
    "# send data through model in minibatches for 10 epochs\n",
    "for epoch in range(10):\n",
    "    for minibatch, target in data: # What is in \"data\"?\n",
    "        model.zero_grad() # pytorch accumulates gradients, making them zero for each minibatch\n",
    "        \n",
    "        #forward pass\n",
    "        out = model(autograd.Variable(minibatch))\n",
    "        \n",
    "        #backward pass \n",
    "        L = loss_func(out,target) #calculate loss\n",
    "        L.backward() # calculate gradients\n",
    "        optimizer.step() # make an update step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
