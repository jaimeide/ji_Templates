{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great References to study deeper:\n",
    "\n",
    "- David Silver (DeepMind)\n",
    "http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html\n",
    "- Richard Sutton\n",
    "http://incompleteideas.net/book/the-book-2nd.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In WinEluk use environment \"cvision\"\n",
    "# In Valta use root environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 0.4.0\n",
      "Cuda:  True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "print('torch version:',torch.__version__)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print('Cuda: ', use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CartPole-v0: Experience Replay + \"Q-learning\" + Neural Net (2 layers)\n",
    "- **GOAL**: be able to keep the pole for >195 steps, that is, reward>195\n",
    "\n",
    "- Ref: hhttps://arxiv.org/abs/1312.5602\n",
    "      \n",
    "- **PROBLEM** of previous implementation:\n",
    "-   a) samples are highly correlated --> solved with experience replay (OK)\n",
    "-   b) non-stationary distribution -->  \n",
    "- **SOLUTION**: a buffer of past experience (works like a batch), so that the learning is made based on the history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\n",
      "*** Episode 0 ***                 \n",
      " Avg.Reward [last 50]: 0.22, [last 100]: 0.11, [all]: 11.00                \n",
      " epsilon: 0.88, cont_steps= 11\n",
      "Elapsed time:  00:00:00\n",
      "\n",
      "*** Episode 50 ***                 \n",
      " Avg.Reward [last 50]: 112.76, [last 100]: 56.49, [all]: 110.76                \n",
      " epsilon: 0.02, cont_steps= 5649\n",
      "Elapsed time:  00:00:11\n",
      "\n",
      "*** Episode 100 ***                 \n",
      " Avg.Reward [last 50]: 189.28, [last 100]: 151.02, [all]: 149.63                \n",
      " epsilon: 0.02, cont_steps= 15113\n",
      "Elapsed time:  00:00:34\n",
      "\n",
      "*** Episode 150 ***                 \n",
      " Avg.Reward [last 50]: 191.22, [last 100]: 190.25, [all]: 163.40                \n",
      " epsilon: 0.02, cont_steps= 24674\n",
      "Elapsed time:  00:00:58\n",
      "\n",
      "*** Episode 200 ***                 \n",
      " Avg.Reward [last 50]: 198.02, [last 100]: 194.62, [all]: 172.01                \n",
      " epsilon: 0.02, cont_steps= 34575\n",
      "Elapsed time:  00:01:17\n",
      "SOLVED! After 205 episodes\n",
      "\n",
      "*** Episode 250 ***                 \n",
      " Avg.Reward [last 50]: 198.50, [last 100]: 198.26, [all]: 177.29                \n",
      " epsilon: 0.02, cont_steps= 44500\n",
      "Elapsed time:  00:01:41\n",
      "\n",
      "*** Episode 300 ***                 \n",
      " Avg.Reward [last 50]: 191.72, [last 100]: 195.11, [all]: 179.69                \n",
      " epsilon: 0.02, cont_steps= 54086\n",
      "Elapsed time:  00:01:59\n",
      "\n",
      "*** Episode 350 ***                 \n",
      " Avg.Reward [last 50]: 192.74, [last 100]: 192.23, [all]: 181.55                \n",
      " epsilon: 0.02, cont_steps= 63723\n",
      "Elapsed time:  00:02:20\n",
      "\n",
      "*** Episode 400 ***                 \n",
      " Avg.Reward [last 50]: 192.92, [last 100]: 192.83, [all]: 182.97                \n",
      " epsilon: 0.02, cont_steps= 73369\n",
      "Elapsed time:  00:02:41\n",
      "\n",
      "*** Episode 450 ***                 \n",
      " Avg.Reward [last 50]: 193.94, [last 100]: 193.43, [all]: 184.18                \n",
      " epsilon: 0.02, cont_steps= 83066\n",
      "Elapsed time:  00:03:02\n",
      "Average reward: 185.26\n",
      "Average reward (last 100 episodes): 194.63\n",
      "#Episodes until SOLVE:  205\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAskAAAE/CAYAAAC0Fl50AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhGQAABgRkgEAYOT/A5PUMaDnS7ihAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff292c3d2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Debug mode\n",
    "debug = False\n",
    "\n",
    "# Set CPU or GPU device\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\") \n",
    "\n",
    "# Set Gym environment\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# Set seeds\n",
    "seed_value = 23\n",
    "env.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "\n",
    "# PARAMS\n",
    "num_episodes = 500\n",
    "score_goal = 195 # official\n",
    "\n",
    "gamma = 0.99\n",
    "my_lr = 0.02\n",
    "nhidden = 64\n",
    "\n",
    "replay_mem_size = 50000\n",
    "batch_size = 32\n",
    "\n",
    "egreedy = 0.9\n",
    "egreedy_final = 0.02\n",
    "egreedy_decay = 500\n",
    "\n",
    "# e-Greedy strategy\n",
    "def calc_epsilon(nsteps):\n",
    "    epsilon = egreedy_final + \\\n",
    "       (egreedy - egreedy_final)*math.exp(-1.*nsteps/egreedy_decay) \n",
    "    return epsilon\n",
    "\n",
    "class ExperienceReplay():\n",
    "    # works like a buffer of past experience\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity # maximum size of memory\n",
    "        self.memory = [] # list of states\n",
    "        self.position = 0 # keep track where to add the new sample\n",
    " \n",
    "    def push(self, state, action, new_state, reward, done):\n",
    "        transition = (state, action, new_state, reward, done)\n",
    "        \n",
    "        if self.position >= len(self.memory):\n",
    "            self.memory.append(transition) # keep adding while memory is no full\n",
    "        else:\n",
    "            self.memory[self.position] = transition # add state at \"position\"\n",
    "        \n",
    "        self.position = ( self.position + 1 ) % self.capacity # it works as a cuclic counter\n",
    "        \n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        # zip(*list) --> # this will return the states grouped by each variable\n",
    "        # print([i for i in zip([[1,2,3],[4,5,6]])])\n",
    "        # [([1, 2, 3],), ([4, 5, 6],)]\n",
    "        # print([i for i in zip(*[[1,2,3],[4,5,6]])])\n",
    "        # [(1, 4), (2, 5), (3, 6)]\n",
    "        \n",
    "        # random.sample(self.memory, batch_size) --> return random samples of states\n",
    "        \n",
    "        return zip(*random.sample(self.memory, batch_size)) \n",
    "        # If state=[s,a,s',r,done]\n",
    "        # zip(*) will return:\n",
    "        # [(s1,s2,s3),(a1,a2,a3),(s'1,s'2,s'3),(done1,done2,done3)] \n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "# NEURAL NETWORK\n",
    "n_inputs = env.observation_space.shape[0] \n",
    "n_outputs = env.action_space.n\n",
    "\n",
    "# 1) Define the NN architecture\n",
    "class NeuralNet(nn.Module): # self inherits the class nn.Module\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__() # Call parent's init\n",
    "        self.linear1 = nn.Linear(n_inputs,nhidden) # input\n",
    "        #self.linear2 = nn.Linear(nhidden[0],nhidden[1]) # layer 1\n",
    "        self.linear2 = nn.Linear(nhidden,n_outputs) # layer 2\n",
    "        \n",
    "        self.activation = nn.Tanh()\n",
    "        #self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        output1 = self.linear1(x)\n",
    "        output1 = self.activation(output1)\n",
    "        output2 = self.linear2(output1)\n",
    "        #output2 = self.activation(output2)\n",
    "        #output3 = self.linear3(output2)\n",
    "        \n",
    "        return output2\n",
    "    \n",
    "# 2) Define the NN-agent\n",
    "class Qnet_agent():\n",
    "    # 1) Init\n",
    "    def __init__(self):\n",
    "        # a) Architecture\n",
    "        self.nn = NeuralNet().to(device)\n",
    "        # b) Loss\n",
    "        self.loss = nn.MSELoss() # linear regression\n",
    "        # c) Optim\n",
    "        self.optimizer = optim.Adam(params=self.nn.parameters(),lr=my_lr)\n",
    "        \n",
    "    # 2) Action\n",
    "    def select_action(self,state,epsilon):\n",
    "        # e-greedy with Exploit x Explore trade-off\n",
    "        randx = torch.rand(1)[0]\n",
    "        \n",
    "        if randx > epsilon:\n",
    "            # Exploit\n",
    "            with torch.no_grad(): # more efficient than detach()\n",
    "                state = torch.Tensor(state).to(device)\n",
    "                action_from_nn = self.nn(state)\n",
    "                action_index = torch.max(action_from_nn,0)[1] # 1 for the index\n",
    "                action = action_index.item()\n",
    "                \n",
    "                if debug:\n",
    "                    print('--> Exploit action_from_nn[',action_from_nn,'] action:',action)\n",
    "        else:\n",
    "            # Explore\n",
    "            action = env.action_space.sample()\n",
    "            if debug:\n",
    "                print('--> Explore: action:',action)\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    # 3) Optimize (UPDATED FOR REPLAY)\n",
    "    #def optimize(self,state,action,new_state,reward,done):\n",
    "    def optimize(self):\n",
    "        # only update with have memory enough to sample\n",
    "        if (len(memory) < batch_size):\n",
    "            return\n",
    "        # Get a sample from memory\n",
    "        state,action,new_state,reward,done = memory.sample(batch_size)\n",
    "        \n",
    "        # Convert to appropriate tensors\n",
    "        state = torch.Tensor(state).to(device) # Convert to tensor\n",
    "        new_state = torch.Tensor(new_state).to(device)\n",
    "        #reward = torch.Tensor([reward]).to(device) # since others are list, reward has to be in list format\n",
    "        reward = torch.Tensor(reward).to(device) # reward is a list now!\n",
    "        # We haver to now convert action and reward to tensor!\n",
    "        action = torch.LongTensor(action).to(device) # for integers\n",
    "        done = torch.Tensor(done).to(device)\n",
    "        \n",
    "        #if done: --> done is a vector... so does not make sense to check like this...\n",
    "        #         --> we will use a little trick \"(1-done)\" to control this!\n",
    "        #    target_value = reward # episode is completed\n",
    "        #else:\n",
    "            ## Bellman's equation (- Traditional way -)\n",
    "            # Q[state,action] = reward + gamma*torch.max(Q[new_state])\n",
    "        \n",
    "        # We will use NNet instead to Approx Q (- New way -)\n",
    "        new_state_values = self.nn(new_state).detach() # leave the grad\n",
    "        #max_new_state_values = torch.max(new_state_values)\n",
    "        max_new_state_values = torch.max(new_state_values,1)[0] # max of each column\n",
    "        #target_value = reward + gamma*max_new_state_values\n",
    "        # (1-done) is use as a trick. If done=1 --> target_value = reward\n",
    "        target_value = reward + (1-done)*gamma*max_new_state_values\n",
    "\n",
    "        ## Q-learning: use prediction-error to update Q\n",
    "        # (-- Traditional way --)\n",
    "        # Q[state, action] = (1 - lr) * Q[state, action] \\\n",
    "        #    + lr * (reward + gamma * torch.max(Q[new_state]))\n",
    "        \n",
    "        # (-- New way --)    \n",
    "        # Current state\n",
    "        #predicted_value = self.nn(state)[action] # here we carry grad because it will update\n",
    "        # (version for vector: details on how to set the dimensions!)\n",
    "    \n",
    "        #print('actions:',action)\n",
    "        #print('output net:',self.nn(state))\n",
    "        predicted_value = self.nn(state).gather(1,action.unsqueeze(1)).squeeze(1)\n",
    "        #print('reward predictions:',predicted_value)\n",
    "        \n",
    "        # Prediction error\n",
    "        loss = self.loss(predicted_value,target_value)\n",
    "        # Backprop\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step() # update params\n",
    "        \n",
    "\n",
    "# 3) RL part\n",
    "memory = ExperienceReplay(replay_mem_size)\n",
    "\n",
    "agent = Qnet_agent()\n",
    "steps_total = []\n",
    "all_solved_nepisodes = []\n",
    "solved = False\n",
    "cont_steps = 0\n",
    "tic = time.time()\n",
    "for i_episode in range(num_episodes):\n",
    "    \n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        step += 1\n",
    "        \n",
    "        cont_steps += 1 # will keep across episodes\n",
    "        epsilon = calc_epsilon(cont_steps) \n",
    "        \n",
    "        # Action is selected\n",
    "        action = agent.select_action(state,epsilon)\n",
    "        \n",
    "        # Next environment\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        # States: [cart position,cart velocity,pole angle, pole velocity]\n",
    "        \n",
    "        # Update NNet\n",
    "        #agent.optimize(state,action,new_state,reward,done)\n",
    "        memory.push(state, action, new_state, reward, done)\n",
    "        agent.optimize()\n",
    "        \n",
    "        if debug:\n",
    "            print('Step#%d: %s, state:%s'%(step,action,new_state))\n",
    "            #print(info)\n",
    "            env.render()\n",
    "            input(\"\")\n",
    "        \n",
    "        # Prepare next iteration\n",
    "        state = new_state\n",
    "        \n",
    "        if done:\n",
    "            steps_total.append(step)\n",
    "            #print(\"Episode finished after %i steps\" % step )\n",
    "            \n",
    "            avgReward100 = sum(steps_total[-100:])/100\n",
    "            \n",
    "            if avgReward100 > score_goal and not solved:\n",
    "                print('SOLVED! After %d episodes' %(i_episode))\n",
    "                all_solved_nepisodes.append(i_episode)\n",
    "                solved = True\n",
    "                break\n",
    "            \n",
    "            # Better reporting\n",
    "            interval = 50\n",
    "            if i_episode % interval == 0:\n",
    "                print('\\n*** Episode %i *** \\\n",
    "                \\n Avg.Reward [last %i]: %.2f, [last 100]: %.2f, [all]: %.2f\\\n",
    "                \\n epsilon: %.2f, cont_steps= %d'%\n",
    "                      (i_episode, interval,\n",
    "                      sum(steps_total[-interval:])/interval,\n",
    "                      avgReward100,\n",
    "                      sum(steps_total)/len(steps_total),\n",
    "                       epsilon, cont_steps\n",
    "                      ))\n",
    "                toc = time.time()\n",
    "                print(\"Elapsed time: \", time.strftime(\"%H:%M:%S\", time.gmtime(toc-tic)))\n",
    "            \n",
    "            break\n",
    "        \n",
    "\n",
    "print(\"Average reward: %.2f\" % (sum(steps_total)/num_episodes))\n",
    "print(\"Average reward (last 100 episodes): %.2f\" % (sum(steps_total[-100:])/100))\n",
    "print(\"#Episodes until SOLVE: \", all_solved_nepisodes[0])\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.title(\"Rewards\")\n",
    "plt.bar(torch.arange(len(steps_total)), steps_total, alpha=0.6, color='green')\n",
    "plt.show()\n",
    "\n",
    "env.close()\n",
    "env.env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
